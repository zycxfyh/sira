name: ğŸš€ Performance Tests

on:
  push:
    branches: [ main, master, develop ]
  pull_request:
    branches: [ main, master, develop ]
    paths:
      - 'src/**'
      - 'scripts/**'
      - 'package.json'
      - '.github/workflows/performance-tests.yml'
  workflow_dispatch:
    inputs:
      duration:
        description: 'Performance test duration (seconds)'
        required: false
        default: '60'
        type: string
      connections:
        description: 'Number of concurrent connections'
        required: false
        default: '10'
        type: string

jobs:
  performance-test:
    name: Performance Benchmark
    runs-on: ubuntu-latest
    timeout-minutes: 15

    strategy:
      matrix:
        node-version: [18.x, 20.x]

    steps:
    - name: ğŸ“¥ Checkout code
      uses: actions/checkout@v5

    - name: ğŸŸ¢ Setup Node.js ${{ matrix.node-version }}
      uses: actions/setup-node@v4
      with:
        node-version: ${{ matrix.node-version }}
        cache: 'npm'

    - name: ğŸ“¦ Install dependencies
      run: npm ci

    - name: ğŸ”¨ Build application
      run: npm run build 2>/dev/null || echo "No build script found, skipping"

    - name: ğŸ§ª Run unit tests
      run: npm test -- --passWithNoTests --coverage=false
      continue-on-error: true

    - name: âš¡ Start application for performance testing
      run: |
        npm start &
        SERVER_PID=$!
        echo "SERVER_PID=$SERVER_PID" >> $GITHUB_ENV

        # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
        for i in {1..30}; do
          if curl -f http://localhost:8080/health >/dev/null 2>&1; then
            echo "âœ… Server started successfully"
            break
          fi
          if [ $i -eq 30 ]; then
            echo "âŒ Server failed to start"
            kill $SERVER_PID 2>/dev/null || true
            exit 1
          fi
          echo "â³ Waiting for server to start... ($i/30)"
          sleep 2
        done

    - name: ğŸ“Š Run performance benchmark
      run: |
        # è®¾ç½®æµ‹è¯•å‚æ•°
        DURATION=${{ github.event.inputs.duration || '60' }}
        CONNECTIONS=${{ github.event.inputs.connections || '10' }}

        echo "ğŸ¯ Running performance test with:"
        echo "  Duration: ${DURATION}s"
        echo "  Connections: ${CONNECTIONS}"
        echo "  Node version: ${{ matrix.node-version }}"

        # è¿è¡Œæ€§èƒ½æµ‹è¯•
        npm run test:performance

        # æ£€æŸ¥æ€§èƒ½æŠ¥å‘Šæ˜¯å¦å­˜åœ¨
        if [ -f "performance-report.json" ]; then
          echo "âœ… Performance report generated"
        else
          echo "âŒ Performance report not found"
          exit 1
        fi

    - name: ğŸ“ˆ Analyze performance results
      id: analyze
      run: |
        if [ ! -f "performance-report.json" ]; then
          echo "âŒ Performance report not found"
          exit 1
        fi

        # è¯»å–æ€§èƒ½æŠ¥å‘Š
        RESPONSE_TIME_AVG=$(jq -r '.results.responseTime.endpoints[]? | select(.name == "AIèŠå¤©API") | .avgTime // "0"' performance-report.json)
        THROUGHPUT=$(jq -r '.results.throughput.requests // 0' performance-report.json)
        LATENCY_P95=$(jq -r '.results.throughput.latency.p95 // 0' performance-report.json)
        MEMORY_USAGE=$(jq -r '.results.memory.final.heapUsed // "0MB"' performance-report.json)

        echo "ğŸ“Š Performance Results:"
        echo "  Average Response Time: ${RESPONSE_TIME_AVG}ms"
        echo "  Throughput: ${THROUGHPUT} requests"
        echo "  P95 Latency: ${LATENCY_P95}ms"
        echo "  Memory Usage: ${MEMORY_USAGE}"

        # è®¾ç½®è¾“å‡ºå˜é‡ä¾›åç»­æ­¥éª¤ä½¿ç”¨
        echo "response_time_avg=${RESPONSE_TIME_AVG}" >> $GITHUB_OUTPUT
        echo "throughput=${THROUGHPUT}" >> $GITHUB_OUTPUT
        echo "latency_p95=${LATENCY_P95}" >> $GITHUB_OUTPUT
        echo "memory_usage=${MEMORY_USAGE}" >> $GITHUB_OUTPUT

        # æ€§èƒ½é˜ˆå€¼æ£€æŸ¥
        if (( $(echo "$RESPONSE_TIME_AVG > 1000" | bc -l) )); then
          echo "âš ï¸  Warning: Average response time (${RESPONSE_TIME_AVG}ms) exceeds 1000ms threshold"
        fi

        if (( $(echo "$LATENCY_P95 > 2000" | bc -l) )); then
          echo "âš ï¸  Warning: P95 latency (${LATENCY_P95}ms) exceeds 2000ms threshold"
        fi

    - name: ğŸ“‹ Generate performance report
      run: |
        cat > performance-summary.md << 'EOF'
        # ğŸš€ Performance Test Results

        ## Test Configuration
        - **Node.js Version**: ${{ matrix.node-version }}
        - **Test Duration**: ${{ github.event.inputs.duration || '60' }} seconds
        - **Concurrent Connections**: ${{ github.event.inputs.connections || '10' }}
        - **Timestamp**: $(date -u +"%Y-%m-%d %H:%M:%S UTC")

        ## Key Metrics

        | Metric | Value | Status |
        |--------|-------|--------|
        | Average Response Time | ${{ steps.analyze.outputs.response_time_avg }}ms | $(if (( $(echo "${{ steps.analyze.outputs.response_time_avg }} > 1000" | bc -l) )); then echo "âš ï¸ Slow"; else echo "âœ… Good"; fi) |
        | Throughput | ${{ steps.analyze.outputs.throughput }} requests | âœ… |
        | P95 Latency | ${{ steps.analyze.outputs.latency_p95 }}ms | $(if (( $(echo "${{ steps.analyze.outputs.latency_p95 }} > 2000" | bc -l) )); then echo "âš ï¸ High"; else echo "âœ… Good"; fi) |
        | Memory Usage | ${{ steps.analyze.outputs.memory_usage }} | âœ… |

        ## Detailed Results
        \`\`\`json
        $(cat performance-report.json)
        \`\`\`

        ---
        *Generated by Sira AI Gateway Performance Tests*
        EOF

    - name: ğŸ’¾ Upload performance artifacts
      uses: actions/upload-artifact@v4
      with:
        name: performance-report-node-${{ matrix.node-version }}
        path: |
          performance-report.json
          performance-summary.md
        retention-days: 30

    - name: ğŸ“¢ Comment performance results on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const summary = fs.readFileSync('performance-summary.md', 'utf8');

          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: summary
          });

    - name: ğŸ›‘ Stop application
      if: always()
      run: |
        if [ ! -z "$SERVER_PID" ]; then
          echo "Stopping server (PID: $SERVER_PID)"
          kill $SERVER_PID 2>/dev/null || true

          # ç­‰å¾…è¿›ç¨‹å®Œå…¨åœæ­¢
          for i in {1..10}; do
            if ! kill -0 $SERVER_PID 2>/dev/null; then
              echo "âœ… Server stopped successfully"
              break
            fi
            sleep 1
          done

          # å¼ºåˆ¶ç»ˆæ­¢å¦‚æœè¿˜æ²¡åœæ­¢
          kill -9 $SERVER_PID 2>/dev/null || true
        fi

    - name: âš ï¸ Performance regression check
      if: github.event_name == 'pull_request'
      run: |
        echo "ğŸ” Checking for performance regressions..."

        # ä¸‹è½½åŸºå‡†æ€§èƒ½æ•°æ®
        if [ -f "performance-baselines/main.json" ]; then
          echo "âœ… Found baseline performance data"
        else
          echo "âš ï¸  No baseline performance data found, creating from current results"
          mkdir -p performance-baselines
          cp performance-report.json performance-baselines/main.json
        fi

        # è¿è¡Œæ€§èƒ½å›å½’æ£€æµ‹
        npm run test:performance:regression

        echo "âœ… Performance regression check completed"
