/**
 * Sira AIç½‘å…³ - æ€§èƒ½åŸºå‡†æµ‹è¯•ç®¡ç†æ¨¡å—
 * æä¾›å…¨é¢çš„AIæ¨¡å‹æ€§èƒ½è¯„ä¼°ç³»ç»Ÿ
 */

const EventEmitter = require('events')
const fs = require('fs').promises
const path = require('path')
const { performance } = require('perf_hooks')

class PerformanceBenchmarkManager extends EventEmitter {
  constructor (options = {}) {
    super()

    this.options = {
      resultsDir: options.resultsDir || path.join(process.cwd(), 'benchmark-results'),
      maxConcurrency: options.maxConcurrency || 5,
      defaultIterations: options.defaultIterations || 5,
      timeout: options.timeout || 30000, // 30ç§’è¶…æ—¶
      enableDetailedLogging: options.enableDetailedLogging || false,
      ...options
    }

    // æµ‹è¯•ç»“æœå­˜å‚¨
    this.results = new Map()
    this.activeTests = new Set()

    // æŒ‡æ ‡è®¡ç®—å™¨
    this.metricsCalculator = new MetricsCalculator()

    // æµ‹è¯•æ‰§è¡Œå™¨
    this.testExecutor = new TestExecutor(this.options)

    this.initializeResultsDirectory()
    this.emit('initialized')
    console.log('âœ… æ€§èƒ½åŸºå‡†æµ‹è¯•ç®¡ç†æ¨¡å—åˆå§‹åŒ–å®Œæˆ')
  }

  /**
     * åˆå§‹åŒ–ç»“æœç›®å½•
     */
  async initializeResultsDirectory () {
    try {
      await fs.mkdir(this.options.resultsDir, { recursive: true })
      console.log(`ğŸ“ åŸºå‡†æµ‹è¯•ç»“æœç›®å½•: ${this.options.resultsDir}`)
    } catch (error) {
      console.error('åˆ›å»ºç»“æœç›®å½•å¤±è´¥:', error)
    }
  }

  /**
     * è¿è¡ŒåŸºå‡†æµ‹è¯•
     */
  async runBenchmark (config) {
    const testId = this.generateTestId()
    const startTime = performance.now()

    try {
      this.activeTests.add(testId)

      const testConfig = this.normalizeConfig(config)
      console.log(`ğŸš€ å¼€å§‹åŸºå‡†æµ‹è¯•: ${testId}`)

      // éªŒè¯é…ç½®
      this.validateBenchmarkConfig(testConfig)

      // æ‰§è¡Œæµ‹è¯•
      const results = await this.executeBenchmark(testConfig, testId)

      // åˆ†æç»“æœ
      const analysis = this.analyzeResults(results, testConfig)

      // ä¿å­˜ç»“æœ
      const testResult = {
        testId,
        config: testConfig,
        results,
        analysis,
        metadata: {
          startTime: new Date(startTime).toISOString(),
          endTime: new Date().toISOString(),
          duration: performance.now() - startTime,
          version: '1.0.0'
        }
      }

      await this.saveResults(testResult)
      this.results.set(testId, testResult)

      this.emit('benchmarkCompleted', testResult)
      console.log(`âœ… åŸºå‡†æµ‹è¯•å®Œæˆ: ${testId}`)

      return testResult
    } catch (error) {
      console.error(`âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: ${testId}`, error)
      this.emit('benchmarkFailed', { testId, error: error.message })
      throw error
    } finally {
      this.activeTests.delete(testId)
    }
  }

  /**
     * ç”Ÿæˆæµ‹è¯•ID
     */
  generateTestId () {
    return `benchmark_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`
  }

  /**
     * æ ‡å‡†åŒ–é…ç½®
     */
  normalizeConfig (config) {
    return {
      name: config.name || `Benchmark ${new Date().toLocaleString()}`,
      models: config.models || ['gpt-4', 'deepseek-chat'],
      tasks: config.tasks || ['simple_qa'],
      iterations: config.iterations || this.options.defaultIterations,
      concurrency: config.concurrency || this.options.maxConcurrency,
      timeout: config.timeout || this.options.timeout,
      parameters: config.parameters || {},
      includeQualityAssessment: config.includeQualityAssessment !== false,
      generateReport: config.generateReport !== false,
      ...config
    }
  }

  /**
     * éªŒè¯åŸºå‡†æµ‹è¯•é…ç½®
     */
  validateBenchmarkConfig (config) {
    if (!config.models || config.models.length === 0) {
      throw new Error('è‡³å°‘éœ€è¦æŒ‡å®šä¸€ä¸ªæ¨¡å‹')
    }

    if (!config.tasks || config.tasks.length === 0) {
      throw new Error('è‡³å°‘éœ€è¦æŒ‡å®šä¸€ä¸ªæµ‹è¯•ä»»åŠ¡')
    }

    if (config.iterations < 1) {
      throw new Error('è¿­ä»£æ¬¡æ•°å¿…é¡»å¤§äº0')
    }

    if (config.concurrency < 1 || config.concurrency > 20) {
      throw new Error('å¹¶å‘æ•°å¿…é¡»åœ¨1-20ä¹‹é—´')
    }
  }

  /**
     * æ‰§è¡ŒåŸºå‡†æµ‹è¯•
     */
  async executeBenchmark (config, testId) {
    const results = {
      models: {},
      tasks: {},
      summary: {}
    }

    // ä¸ºæ¯ä¸ªæ¨¡å‹æ‰§è¡Œæµ‹è¯•
    for (const model of config.models) {
      console.log(`ğŸ¤– æµ‹è¯•æ¨¡å‹: ${model}`)
      results.models[model] = await this.testExecutor.runModelTests(
        model,
        config.tasks,
        config.iterations,
        config,
        testId
      )
    }

    // æŒ‰ä»»åŠ¡èšåˆç»“æœ
    for (const task of config.tasks) {
      results.tasks[task] = this.aggregateTaskResults(results.models, task)
    }

    // ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
    results.summary = this.generateSummaryStats(results, config)

    return results
  }

  /**
     * èšåˆä»»åŠ¡ç»“æœ
     */
  aggregateTaskResults (modelResults, task) {
    const taskResults = {
      model_performance: {},
      averages: {},
      best_performer: null,
      worst_performer: null
    }

    for (const [model, results] of Object.entries(modelResults)) {
      if (results.tasks && results.tasks[task]) {
        taskResults.model_performance[model] = results.tasks[task]
      }
    }

    // è®¡ç®—å¹³å‡å€¼
    const metrics = ['response_time', 'tokens_used', 'cost', 'quality_score']
    for (const metric of metrics) {
      const values = Object.values(taskResults.model_performance)
        .map(r => r[metric])
        .filter(v => v !== undefined && v !== null)

      if (values.length > 0) {
        taskResults.averages[metric] = {
          mean: values.reduce((a, b) => a + b, 0) / values.length,
          min: Math.min(...values),
          max: Math.max(...values),
          std: this.calculateStd(values)
        }
      }
    }

    // æ‰¾å‡ºæœ€ä½³å’Œæœ€å·®è¡¨ç°è€…
    const performances = Object.entries(taskResults.model_performance)
    if (performances.length > 0) {
      taskResults.best_performer = performances.reduce((best, [model, perf]) =>
        perf.response_time < best.perf.response_time ? { model, perf } : best,
      { model: performances[0][0], perf: performances[0][1] }
      )

      taskResults.worst_performer = performances.reduce((worst, [model, perf]) =>
        perf.response_time > worst.perf.response_time ? { model, perf } : worst,
      { model: performances[0][0], perf: performances[0][1] }
      )
    }

    return taskResults
  }

  /**
     * ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
     */
  generateSummaryStats (results, config) {
    const summary = {
      total_tests: 0,
      total_duration: 0,
      average_response_time: 0,
      average_cost: 0,
      average_quality: 0,
      cost_efficiency_rankings: [],
      performance_rankings: [],
      quality_rankings: []
    }

    const allPerformances = []

    for (const [model, modelResults] of Object.entries(results.models)) {
      if (modelResults.summary) {
        summary.total_tests += modelResults.summary.total_requests || 0
        summary.total_duration += modelResults.summary.total_duration || 0

        allPerformances.push({
          model,
          response_time: modelResults.summary.average_response_time,
          cost: modelResults.summary.average_cost,
          quality: modelResults.summary.average_quality_score,
          cost_efficiency: modelResults.summary.cost_per_token
        })
      }
    }

    if (allPerformances.length > 0) {
      summary.average_response_time = allPerformances.reduce((sum, p) => sum + p.response_time, 0) / allPerformances.length
      summary.average_cost = allPerformances.reduce((sum, p) => sum + p.cost, 0) / allPerformances.length
      summary.average_quality = allPerformances.reduce((sum, p) => sum + (p.quality || 0), 0) / allPerformances.length

      // ç”Ÿæˆæ’å
      summary.performance_rankings = allPerformances
        .sort((a, b) => a.response_time - b.response_time)
        .map(p => ({ model: p.model, value: p.response_time }))

      summary.cost_efficiency_rankings = allPerformances
        .filter(p => p.cost_efficiency)
        .sort((a, b) => a.cost_efficiency - b.cost_efficiency)
        .map(p => ({ model: p.model, value: p.cost_efficiency }))

      summary.quality_rankings = allPerformances
        .filter(p => p.quality)
        .sort((a, b) => b.quality - a.quality)
        .map(p => ({ model: p.model, value: p.quality }))
    }

    return summary
  }

  /**
     * åˆ†æç»“æœ
     */
  analyzeResults (results, config) {
    return {
      performance_analysis: this.analyzePerformance(results),
      cost_analysis: this.analyzeCost(results),
      quality_analysis: this.analyzeQuality(results),
      recommendations: this.generateRecommendations(results, config)
    }
  }

  /**
     * æ€§èƒ½åˆ†æ
     */
  analyzePerformance (results) {
    const analysis = {
      fastest_model: null,
      slowest_model: null,
      response_time_distribution: {},
      stability_metrics: {}
    }

    const modelTimes = {}

    for (const [model, modelResults] of Object.entries(results.models)) {
      if (modelResults.summary && modelResults.summary.average_response_time) {
        modelTimes[model] = modelResults.summary.average_response_time
      }
    }

    if (Object.keys(modelTimes).length > 0) {
      const sorted = Object.entries(modelTimes).sort((a, b) => a[1] - b[1])
      analysis.fastest_model = { model: sorted[0][0], time: sorted[0][1] }
      analysis.slowest_model = { model: sorted[sorted.length - 1][0], time: sorted[sorted.length - 1][1] }

      // è®¡ç®—å“åº”æ—¶é—´åˆ†å¸ƒ
      const times = Object.values(modelTimes)
      analysis.response_time_distribution = {
        mean: times.reduce((a, b) => a + b, 0) / times.length,
        median: this.calculateMedian(times),
        p95: this.calculatePercentile(times, 95),
        p99: this.calculatePercentile(times, 99)
      }
    }

    return analysis
  }

  /**
     * æˆæœ¬åˆ†æ
     */
  analyzeCost (results) {
    const analysis = {
      cheapest_model: null,
      most_expensive_model: null,
      cost_distribution: {},
      cost_efficiency_scores: {}
    }

    const modelCosts = {}

    for (const [model, modelResults] of Object.entries(results.models)) {
      if (modelResults.summary && modelResults.summary.average_cost) {
        modelCosts[model] = modelResults.summary.average_cost
      }
    }

    if (Object.keys(modelCosts).length > 0) {
      const sorted = Object.entries(modelCosts).sort((a, b) => a[1] - b[1])
      analysis.cheapest_model = { model: sorted[0][0], cost: sorted[0][1] }
      analysis.most_expensive_model = { model: sorted[sorted.length - 1][0], cost: sorted[sorted.length - 1][1] }

      // è®¡ç®—æˆæœ¬æ•ˆç‡åˆ†æ•°
      for (const [model, cost] of Object.entries(modelCosts)) {
        const quality = results.models[model]?.summary?.average_quality_score || 1
        analysis.cost_efficiency_scores[model] = quality / cost
      }
    }

    return analysis
  }

  /**
     * è´¨é‡åˆ†æ
     */
  analyzeQuality (results) {
    const analysis = {
      highest_quality_model: null,
      lowest_quality_model: null,
      quality_distribution: {},
      consistency_scores: {}
    }

    const modelQualities = {}

    for (const [model, modelResults] of Object.entries(results.models)) {
      if (modelResults.summary && modelResults.summary.average_quality_score) {
        modelQualities[model] = modelResults.summary.average_quality_score
      }
    }

    if (Object.keys(modelQualities).length > 0) {
      const sorted = Object.entries(modelQualities).sort((a, b) => b[1] - a[1])
      analysis.highest_quality_model = { model: sorted[0][0], quality: sorted[0][1] }
      analysis.lowest_quality_model = { model: sorted[sorted.length - 1][0], quality: sorted[sorted.length - 1][1] }

      // è®¡ç®—è´¨é‡åˆ†å¸ƒ
      const qualities = Object.values(modelQualities)
      analysis.quality_distribution = {
        mean: qualities.reduce((a, b) => a + b, 0) / qualities.length,
        median: this.calculateMedian(qualities),
        std: this.calculateStd(qualities)
      }
    }

    return analysis
  }

  /**
     * ç”Ÿæˆæ¨è
     */
  generateRecommendations (results, config) {
    const recommendations = {
      best_overall: null,
      best_for_speed: null,
      best_for_cost: null,
      best_for_quality: null,
      suggestions: []
    }

    const modelScores = {}

    for (const [model, modelResults] of Object.entries(results.models)) {
      if (modelResults.summary) {
        const summary = modelResults.summary
        modelScores[model] = {
          speed_score: summary.average_response_time ? 1 / summary.average_response_time : 0,
          cost_score: summary.average_cost ? 1 / summary.average_cost : 0,
          quality_score: summary.average_quality_score || 0
        }
      }
    }

    // è®¡ç®—ç»¼åˆå¾—åˆ† (å½’ä¸€åŒ–åå¹³å‡)
    for (const [model, scores] of Object.entries(modelScores)) {
      const normalizedScores = this.normalizeScores(scores)
      modelScores[model].overall_score = (normalizedScores.speed_score + normalizedScores.cost_score + normalizedScores.quality_score) / 3
    }

    if (Object.keys(modelScores).length > 0) {
      // æœ€ä½³ç»¼åˆè¡¨ç°
      const bestOverall = Object.entries(modelScores)
        .sort((a, b) => b[1].overall_score - a[1].overall_score)[0]
      recommendations.best_overall = bestOverall[0]

      // æœ€ä½³é€Ÿåº¦
      const bestSpeed = Object.entries(modelScores)
        .sort((a, b) => b[1].speed_score - a[1].speed_score)[0]
      recommendations.best_for_speed = bestSpeed[0]

      // æœ€ä½³æˆæœ¬æ•ˆç›Š
      const bestCost = Object.entries(modelScores)
        .sort((a, b) => b[1].cost_score - a[1].cost_score)[0]
      recommendations.best_for_cost = bestCost[0]

      // æœ€ä½³è´¨é‡
      const bestQuality = Object.entries(modelScores)
        .sort((a, b) => b[1].quality_score - a[1].quality_score)[0]
      recommendations.best_for_quality = bestQuality[0]
    }

    // ç”Ÿæˆå»ºè®®
    recommendations.suggestions = this.generateSuggestions(results, config)

    return recommendations
  }

  /**
     * ç”Ÿæˆå»ºè®®
     */
  generateSuggestions (results, config) {
    const suggestions = []

    // åŸºäºæ€§èƒ½çš„å»ºè®®
    const perfAnalysis = this.analyzePerformance(results)
    if (perfAnalysis.fastest_model && perfAnalysis.slowest_model) {
      const speedup = perfAnalysis.slowest_model.time / perfAnalysis.fastest_model.time
      if (speedup > 2) {
        suggestions.push(`è€ƒè™‘ä½¿ç”¨ ${perfAnalysis.fastest_model.model} æ›¿ä»£ ${perfAnalysis.slowest_model.model} å¯æå‡ ${Math.round((speedup - 1) * 100)}% çš„å“åº”é€Ÿåº¦`)
      }
    }

    // åŸºäºæˆæœ¬çš„å»ºè®®
    const costAnalysis = this.analyzeCost(results)
    if (costAnalysis.cheapest_model && costAnalysis.most_expensive_model) {
      const savings = costAnalysis.most_expensive_model.cost / costAnalysis.cheapest_model.cost
      if (savings > 1.5) {
        suggestions.push(`åˆ‡æ¢åˆ° ${costAnalysis.cheapest_model.model} å¯èŠ‚çœ ${Math.round((savings - 1) * 100)}% çš„æˆæœ¬`)
      }
    }

    // åŸºäºè´¨é‡çš„å»ºè®®
    const qualityAnalysis = this.analyzeQuality(results)
    if (qualityAnalysis.highest_quality_model && qualityAnalysis.quality_distribution.std > 0.3) {
      suggestions.push(`å¯¹äºé«˜è´¨é‡è¦æ±‚ä»»åŠ¡ï¼Œæ¨èä½¿ç”¨ ${qualityAnalysis.highest_quality_model.model}`)
    }

    return suggestions
  }

  /**
     * ä¿å­˜ç»“æœ
     */
  async saveResults (testResult) {
    const filename = `benchmark_${testResult.testId}_${new Date().toISOString().slice(0, 19).replace(/:/g, '-')}.json`
    const filepath = path.join(this.options.resultsDir, filename)

    try {
      await fs.writeFile(filepath, JSON.stringify(testResult, null, 2), 'utf8')
      console.log(`ğŸ’¾ ç»“æœå·²ä¿å­˜: ${filepath}`)
    } catch (error) {
      console.error('ä¿å­˜ç»“æœå¤±è´¥:', error)
    }
  }

  /**
     * è·å–æµ‹è¯•ç»“æœ
     */
  getResults (testId = null) {
    if (testId) {
      return this.results.get(testId) || null
    }
    return Array.from(this.results.values())
  }

  /**
     * è·å–æœ€æ–°ç»“æœ
     */
  getLatestResults (limit = 10) {
    return Array.from(this.results.values())
      .sort((a, b) => new Date(b.metadata.startTime) - new Date(a.metadata.startTime))
      .slice(0, limit)
  }

  /**
     * æ¯”è¾ƒæ¨¡å‹æ€§èƒ½
     */
  compareModels (models, metric = 'response_time') {
    const comparison = {
      metric,
      rankings: [],
      differences: {}
    }

    const modelValues = {}

    for (const model of models) {
      const latestResult = this.getLatestResults(1).find(r =>
        r.config.models.includes(model)
      )

      if (latestResult && latestResult.results.models[model]) {
        const summary = latestResult.results.models[model].summary
        if (summary) {
          switch (metric) {
            case 'response_time':
              modelValues[model] = summary.average_response_time
              break
            case 'cost':
              modelValues[model] = summary.average_cost
              break
            case 'quality':
              modelValues[model] = summary.average_quality_score
              break
            case 'cost_efficiency':
              modelValues[model] = summary.cost_per_token
              break
          }
        }
      }
    }

    // ç”Ÿæˆæ’å
    comparison.rankings = Object.entries(modelValues)
      .sort((a, b) => {
        // å¯¹äºæˆæœ¬å’Œå“åº”æ—¶é—´ï¼Œè¶Šå°è¶Šå¥½ï¼›å¯¹äºè´¨é‡å’Œæ•ˆç‡ï¼Œè¶Šå¤§è¶Šå¥½
        if (metric === 'response_time' || metric === 'cost' || metric === 'cost_efficiency') {
          return a[1] - b[1]
        } else {
          return b[1] - a[1]
        }
      })
      .map(([model, value]) => ({ model, value }))

    // è®¡ç®—å·®å¼‚
    if (comparison.rankings.length >= 2) {
      const best = comparison.rankings[0]
      const worst = comparison.rankings[comparison.rankings.length - 1]

      comparison.differences = {
        best_to_worst: metric === 'response_time' || metric === 'cost'
          ? `${((worst.value / best.value - 1) * 100).toFixed(1)}% å·®å¼‚`
          : `${((best.value / worst.value - 1) * 100).toFixed(1)}% å·®å¼‚`,
        improvement_potential: `åˆ‡æ¢åˆ° ${best.model} å¯è·å¾—æ˜¾è‘—æå‡`
      }
    }

    return comparison
  }

  /**
     * å·¥å…·å‡½æ•°
     */
  calculateStd (values) {
    const mean = values.reduce((a, b) => a + b, 0) / values.length
    const squareDiffs = values.map(value => Math.pow(value - mean, 2))
    const avgSquareDiff = squareDiffs.reduce((a, b) => a + b, 0) / squareDiffs.length
    return Math.sqrt(avgSquareDiff)
  }

  calculateMedian (values) {
    const sorted = [...values].sort((a, b) => a - b)
    const mid = Math.floor(sorted.length / 2)
    return sorted.length % 2 !== 0 ? sorted[mid] : (sorted[mid - 1] + sorted[mid]) / 2
  }

  calculatePercentile (values, percentile) {
    const sorted = [...values].sort((a, b) => a - b)
    const index = (percentile / 100) * (sorted.length - 1)
    const lower = Math.floor(index)
    const upper = Math.ceil(index)
    const weight = index % 1

    if (upper >= sorted.length) return sorted[sorted.length - 1]
    return sorted[lower] * (1 - weight) + sorted[upper] * weight
  }

  normalizeScores (scores) {
    const normalized = {}
    for (const [key, value] of Object.entries(scores)) {
      // ç®€å•çš„å½’ä¸€åŒ–ï¼Œç¡®ä¿æ‰€æœ‰åˆ†æ•°åœ¨0-1èŒƒå›´å†…
      normalized[key] = Math.max(0, Math.min(1, value / 100))
    }
    return normalized
  }

  /**
     * å¯¼å‡ºç»“æœ
     */
  exportResults (format = 'json') {
    const allResults = this.getResults()

    switch (format) {
      case 'json':
        return JSON.stringify(allResults, null, 2)
      case 'csv':
        return this.convertToCSV(allResults)
      default:
        throw new Error(`ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: ${format}`)
    }
  }

  /**
     * è½¬æ¢ä¸ºCSV
     */
  convertToCSV (results) {
    const csv = ['Test ID,Model,Task,Response Time,Cost,Quality Score,Status']

    for (const result of results) {
      for (const [model, modelResults] of Object.entries(result.results.models)) {
        for (const [task, taskResults] of Object.entries(modelResults.tasks || {})) {
          csv.push([
            result.testId,
            model,
            task,
            taskResults.response_time || '',
            taskResults.cost || '',
            taskResults.quality_score || '',
            taskResults.status || 'completed'
          ].join(','))
        }
      }
    }

    return csv.join('\n')
  }
}

/**
 * æŒ‡æ ‡è®¡ç®—å™¨
 */
class MetricsCalculator {
  calculateResponseTimeStats (times) {
    if (!times || times.length === 0) return {}

    const sorted = [...times].sort((a, b) => a - b)

    return {
      mean: times.reduce((a, b) => a + b, 0) / times.length,
      median: sorted.length % 2 === 0
        ? (sorted[sorted.length / 2 - 1] + sorted[sorted.length / 2]) / 2
        : sorted[Math.floor(sorted.length / 2)],
      min: sorted[0],
      max: sorted[sorted.length - 1],
      p95: sorted[Math.floor(sorted.length * 0.95)],
      p99: sorted[Math.floor(sorted.length * 0.99)]
    }
  }

  calculateCostEfficiency (cost, quality, tokens) {
    if (!cost || !tokens) return 0
    const costPerToken = cost / tokens
    const qualityBonus = quality || 1
    return qualityBonus / costPerToken
  }

  calculateStabilityScore (times) {
    if (!times || times.length < 2) return 0

    const mean = times.reduce((a, b) => a + b, 0) / times.length
    const variance = times.reduce((sum, time) => sum + Math.pow(time - mean, 2), 0) / times.length
    const std = Math.sqrt(variance)

    // ç¨³å®šæ€§åˆ†æ•°ï¼šæ ‡å‡†å·®è¶Šå°ï¼Œåˆ†æ•°è¶Šé«˜ (0-1)
    return Math.max(0, Math.min(1, 1 - (std / mean)))
  }
}

/**
 * æµ‹è¯•æ‰§è¡Œå™¨
 */
class TestExecutor {
  constructor (options) {
    this.options = options
    this.activeRequests = new Map()
  }

  async runModelTests (model, tasks, iterations, config, testId) {
    const results = {
      model,
      tasks: {},
      summary: {
        total_requests: 0,
        successful_requests: 0,
        failed_requests: 0,
        total_duration: 0,
        average_response_time: 0,
        average_cost: 0,
        average_quality_score: 0,
        cost_per_token: 0
      }
    }

    // å¯¼å…¥æµ‹è¯•ç”¨ä¾‹
    const { testCases } = require('./benchmark-test-cases')

    // ä¸ºæ¯ä¸ªä»»åŠ¡æ‰§è¡Œæµ‹è¯•
    for (const task of tasks) {
      if (!testCases[task]) {
        console.warn(`âš ï¸ æœªçŸ¥ä»»åŠ¡ç±»å‹: ${task}`)
        continue
      }

      console.log(`ğŸ“‹ æ‰§è¡Œä»»åŠ¡: ${task} (${iterations} æ¬¡è¿­ä»£)`)
      results.tasks[task] = await this.runTaskTests(
        model,
        task,
        testCases[task],
        iterations,
        config,
        testId
      )
    }

    // è®¡ç®—æ±‡æ€»ç»Ÿè®¡
    this.calculateModelSummary(results)

    return results
  }

  async runTaskTests (model, task, testCase, iterations, config, testId) {
    const taskResults = {
      iterations: [],
      response_times: [],
      costs: [],
      quality_scores: [],
      tokens_used: [],
      errors: []
    }

    // å¹¶å‘æ‰§è¡Œæµ‹è¯•
    const concurrency = Math.min(config.concurrency, iterations)
    const chunks = this.chunkArray(Array.from({ length: iterations }, (_, i) => i), concurrency)

    for (const chunk of chunks) {
      const promises = chunk.map(async (iteration) => {
        try {
          const result = await this.runSingleTest(
            model,
            task,
            testCase,
            iteration,
            config,
            testId
          )

          taskResults.iterations.push({
            iteration,
            ...result,
            status: 'success'
          })

          if (result.response_time) taskResults.response_times.push(result.response_time)
          if (result.cost) taskResults.costs.push(result.cost)
          if (result.quality_score !== undefined) taskResults.quality_scores.push(result.quality_score)
          if (result.tokens_used) taskResults.tokens_used.push(result.tokens_used)
        } catch (error) {
          taskResults.errors.push({
            iteration,
            error: error.message,
            timestamp: new Date().toISOString()
          })

          taskResults.iterations.push({
            iteration,
            status: 'failed',
            error: error.message
          })
        }
      })

      await Promise.all(promises)

      // æ·»åŠ å°å»¶è¿Ÿé¿å…å¹¶å‘è¿‡é«˜
      await new Promise(resolve => setTimeout(resolve, 100))
    }

    // è®¡ç®—ä»»åŠ¡ç»Ÿè®¡
    return this.calculateTaskStats(taskResults)
  }

  async runSingleTest (model, task, testCase, iteration, config, testId) {
    const startTime = performance.now()

    try {
      // ç”Ÿæˆæµ‹è¯•è¾“å…¥
      const testInput = testCase.generateInput ? testCase.generateInput() : testCase.input

      // æ„å»ºè¯·æ±‚
      const requestBody = {
        model,
        messages: [{ role: 'user', content: testInput }],
        ...config.parameters
      }

      // è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„AIç½‘å…³API
      // ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿä¸€ä¸ªå“åº”
      const mockResponse = await this.simulateAIRequest(requestBody, config.timeout)

      const endTime = performance.now()
      const responseTime = endTime - startTime

      // æ¨¡æ‹Ÿè´¨é‡è¯„ä¼°
      const qualityScore = config.includeQualityAssessment
        ? this.assessQuality(mockResponse.content, testCase.expected_output) : null

      // æ¨¡æ‹Ÿæˆæœ¬è®¡ç®—
      const tokensUsed = mockResponse.content.length / 4 // ç²—ç•¥ä¼°ç®—
      const cost = this.calculateEstimatedCost(model, tokensUsed)

      return {
        response_time: responseTime,
        content: mockResponse.content,
        tokens_used: tokensUsed,
        cost: cost,
        quality_score: qualityScore,
        timestamp: new Date().toISOString()
      }
    } catch (error) {
      const endTime = performance.now()
      throw new Error(`æµ‹è¯•å¤±è´¥: ${error.message} (è€—æ—¶: ${(endTime - startTime).toFixed(2)}ms)`)
    }
  }

  async simulateAIRequest (requestBody, timeout) {
    // æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
    const delay = Math.random() * 1000 + 500 // 500-1500ms
    await new Promise(resolve => setTimeout(resolve, delay))

    // æ¨¡æ‹Ÿå“åº”å†…å®¹
    const responses = [
      'è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„AIå“åº”ï¼Œç”¨äºæ€§èƒ½åŸºå‡†æµ‹è¯•ã€‚æµ‹è¯•å†…å®¹è´¨é‡å’Œå“åº”æ—¶é—´ã€‚',
      'Performance benchmark test response. This simulates a typical AI model output for evaluation purposes.',
      'åŸºå‡†æµ‹è¯•æ¨¡æ‹Ÿå“åº”ã€‚è¯„ä¼°æ¨¡å‹çš„å“åº”é€Ÿåº¦ã€æˆæœ¬æ•ˆç›Šå’Œè¾“å‡ºè´¨é‡ã€‚',
      'Mock response for benchmarking. Used to measure latency, cost, and quality metrics.',
      'AIæ¨¡å‹æ€§èƒ½æµ‹è¯•å“åº”å†…å®¹ã€‚åŒ…å«è¶³å¤Ÿçš„ä¿¡æ¯ç”¨äºè´¨é‡è¯„ä¼°å’Œç»Ÿè®¡åˆ†æã€‚'
    ]

    return {
      content: responses[Math.floor(Math.random() * responses.length)],
      finish_reason: 'stop'
    }
  }

  assessQuality (actualOutput, expectedOutput) {
    if (!expectedOutput) return 0.8 // é»˜è®¤åˆ†æ•°

    // ç®€å•çš„è´¨é‡è¯„ä¼°ï¼ˆå®é™…åº”è¯¥ä½¿ç”¨æ›´å¤æ‚çš„ç®—æ³•ï¼‰
    const similarity = this.calculateTextSimilarity(actualOutput, expectedOutput)
    return Math.max(0.1, Math.min(1.0, similarity))
  }

  calculateTextSimilarity (text1, text2) {
    // ç®€å•çš„ç›¸ä¼¼åº¦è®¡ç®—
    const words1 = new Set(text1.toLowerCase().split(/\s+/))
    const words2 = new Set(text2.toLowerCase().split(/\s+/))

    const intersection = new Set([...words1].filter(x => words2.has(x)))
    const union = new Set([...words1, ...words2])

    return intersection.size / union.size
  }

  calculateEstimatedCost (model, tokens) {
    // ä¼°ç®—æˆæœ¬ï¼ˆå®é™…åº”è¯¥ä»é…ç½®ä¸­è·å–ï¼‰
    const costPerToken = {
      'gpt-4': 0.03,
      'gpt-3.5-turbo': 0.002,
      'claude-3-opus': 0.015,
      'deepseek-chat': 0.001,
      'qwen-max': 0.002
    }

    return (costPerToken[model] || 0.01) * tokens
  }

  calculateTaskStats (taskResults) {
    const stats = {
      total_iterations: taskResults.iterations.length,
      successful_iterations: taskResults.response_times.length,
      failed_iterations: taskResults.errors.length,
      success_rate: 0,
      response_time: {},
      cost: {},
      quality_score: {},
      tokens_used: {},
      status: 'completed'
    }

    stats.success_rate = stats.successful_iterations / stats.total_iterations

    if (taskResults.response_times.length > 0) {
      stats.response_time = this.calculateStats(taskResults.response_times)
    }

    if (taskResults.costs.length > 0) {
      stats.cost = this.calculateStats(taskResults.costs)
    }

    if (taskResults.quality_scores.length > 0) {
      stats.quality_score = this.calculateStats(taskResults.quality_scores)
    }

    if (taskResults.tokens_used.length > 0) {
      stats.tokens_used = this.calculateStats(taskResults.tokens_used)
    }

    return stats
  }

  calculateStats (values) {
    if (values.length === 0) return {}

    const sorted = [...values].sort((a, b) => a - b)
    const sum = values.reduce((a, b) => a + b, 0)
    const mean = sum / values.length

    const variance = values.reduce((sum, value) => sum + Math.pow(value - mean, 2), 0) / values.length
    const std = Math.sqrt(variance)

    return {
      mean,
      median: sorted.length % 2 === 0
        ? (sorted[sorted.length / 2 - 1] + sorted[sorted.length / 2]) / 2
        : sorted[Math.floor(sorted.length / 2)],
      min: sorted[0],
      max: sorted[sorted.length - 1],
      std,
      p95: sorted[Math.floor(sorted.length * 0.95)] || sorted[sorted.length - 1],
      count: values.length
    }
  }

  calculateModelSummary (results) {
    const summary = results.summary
    let totalResponseTime = 0
    let totalCost = 0
    let totalQuality = 0
    let totalTokens = 0
    let qualityCount = 0

    for (const [taskName, taskResults] of Object.entries(results.tasks)) {
      summary.total_requests += taskResults.total_iterations
      summary.successful_requests += taskResults.successful_iterations
      summary.failed_requests += taskResults.failed_iterations

      if (taskResults.response_time.mean) {
        totalResponseTime += taskResults.response_time.mean
      }

      if (taskResults.cost.mean) {
        totalCost += taskResults.cost.mean
      }

      if (taskResults.quality_score && taskResults.quality_score.mean !== undefined) {
        totalQuality += taskResults.quality_score.mean
        qualityCount++
      }

      if (taskResults.tokens_used.mean) {
        totalTokens += taskResults.tokens_used.mean
      }
    }

    const taskCount = Object.keys(results.tasks).length
    if (taskCount > 0) {
      summary.average_response_time = totalResponseTime / taskCount
      summary.average_cost = totalCost / taskCount
      if (qualityCount > 0) {
        summary.average_quality_score = totalQuality / qualityCount
      }
      if (totalTokens > 0) {
        summary.cost_per_token = totalCost / totalTokens
      }
    }
  }

  chunkArray (array, size) {
    const chunks = []
    for (let i = 0; i < array.length; i += size) {
      chunks.push(array.slice(i, i + size))
    }
    return chunks
  }
}

// åˆ›å»ºå…¨å±€å®ä¾‹
const performanceBenchmarkManager = new PerformanceBenchmarkManager()

// å¯¼å‡ºç±»å’Œå®ä¾‹
module.exports = {
  PerformanceBenchmarkManager,
  performanceBenchmarkManager,
  MetricsCalculator,
  TestExecutor
}
