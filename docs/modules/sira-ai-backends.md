# Sira AI Backends - AIæœåŠ¡é›†æˆ

## æ¦‚è¿°

Sira AI Backends æ˜¯Siraç³»ç»Ÿçš„AIèƒ½åŠ›æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£ä¸å„ç§AIæœåŠ¡æä¾›å•†çš„é›†æˆï¼ŒåŒ…æ‹¬OpenAIã€Anthropicã€Googleç­‰ã€‚å®ƒæä¾›äº†ç»Ÿä¸€çš„AIæœåŠ¡æ¥å£ã€æ™ºèƒ½è·¯ç”±ã€è´Ÿè½½å‡è¡¡å’Œæ•…éšœè½¬ç§»åŠŸèƒ½ï¼Œç¡®ä¿ç³»ç»Ÿèƒ½å¤Ÿé«˜æ•ˆã€å¯é åœ°è®¿é—®å„ç§AIèƒ½åŠ›ã€‚

## æ”¯æŒçš„AIæä¾›å•†

### ğŸ¤– OpenAIé›†æˆ

#### æ”¯æŒçš„æ¨¡å‹
- **GPT-4ç³»åˆ—**: gpt-4, gpt-4-turbo, gpt-4-vision
- **GPT-3.5ç³»åˆ—**: gpt-3.5-turbo, gpt-3.5-turbo-16k
- **åµŒå…¥æ¨¡å‹**: text-embedding-ada-002, text-embedding-3
- **å›¾åƒç”Ÿæˆ**: DALL-E 3, DALL-E 2

#### é…ç½®ç¤ºä¾‹
```rust
#[derive(Debug, Serialize, Deserialize)]
pub struct OpenAIConfig {
    pub api_key: String,
    pub base_url: String,
    pub organization: Option<String>,
    pub models: Vec<String>,
    pub timeout: Duration,
    pub max_retries: u32,
}
```

#### åŠŸèƒ½ç‰¹æ€§
```rust
impl OpenAIProvider {
    // æ–‡æœ¬å¯¹è¯
    pub async fn chat_completion(&self, request: ChatRequest) -> Result<ChatResponse, AiError>;

    // æµå¼å¯¹è¯
    pub async fn chat_completion_stream(&self, request: ChatRequest) -> Result<Pin<Box<dyn Stream<Item = Result<ChatResponse, AiError>> + Send>>, AiError>;

    // æ–‡æœ¬åµŒå…¥
    pub async fn create_embedding(&self, request: EmbeddingRequest) -> Result<EmbeddingResponse, AiError>;

    // å›¾åƒç”Ÿæˆ
    pub async fn create_image(&self, request: ImageRequest) -> Result<ImageResponse, AiError>;
}
```

### ğŸ§  Anthropicé›†æˆ

#### æ”¯æŒçš„æ¨¡å‹
- **Claude 3ç³»åˆ—**: claude-3-opus, claude-3-sonnet, claude-3-haiku
- **Claude 2ç³»åˆ—**: claude-2, claude-2.1
- **Claude Instant**: claude-instant-1

#### ç‹¬æœ‰ç‰¹æ€§
- **æ›´é•¿çš„ä¸Šä¸‹æ–‡çª—å£**: æ”¯æŒ200k tokens
- **æ›´å¥½çš„å®‰å…¨æ€§**: ä¸“é—¨çš„å®‰å…¨è®­ç»ƒ
- **å·¥å…·ä½¿ç”¨**: å¼ºå¤§çš„function callingèƒ½åŠ›

#### è¯·æ±‚è½¬æ¢
```rust
// Anthropicçš„è¯·æ±‚æ ¼å¼è½¬æ¢
impl From<ChatRequest> for AnthropicChatRequest {
    fn from(request: ChatRequest) -> Self {
        AnthropicChatRequest {
            model: map_model_name(&request.model),
            messages: convert_messages(request.messages),
            max_tokens: request.max_tokens.unwrap_or(4096),
            temperature: request.temperature,
            top_p: request.top_p,
            top_k: request.top_k,
            stop_sequences: request.stop,
            system: extract_system_message(&request.messages),
        }
    }
}
```

### ğŸŒ Google AIé›†æˆ

#### æ”¯æŒçš„æœåŠ¡
- **PaLM 2**: æ–‡æœ¬ç”Ÿæˆå’Œå¯¹è¯
- **Gemini**: å¤šæ¨¡æ€AIæ¨¡å‹
- **BERT**: è‡ªç„¶è¯­è¨€ç†è§£
- **T5**: æ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢

#### å¤šæ¨¡æ€æ”¯æŒ
```rust
#[derive(Debug, Serialize, Deserialize)]
pub struct GeminiRequest {
    pub contents: Vec<Content>,
    pub generation_config: GenerationConfig,
    pub safety_settings: Vec<SafetySetting>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct Content {
    pub role: String,
    pub parts: Vec<Part>,
}

#[derive(Debug, Serialize, Deserialize)]
pub enum Part {
    Text { text: String },
    InlineData { mime_type: String, data: String },
    FunctionCall { name: String, args: serde_json::Value },
}
```

## æ™ºèƒ½è·¯ç”±ç³»ç»Ÿ

### ğŸ¯ è·¯ç”±ç­–ç•¥

#### åŸºäºæˆæœ¬çš„è·¯ç”±
```rust
#[derive(Debug)]
pub struct CostBasedRouter {
    providers: Vec<Arc<dyn AiProvider>>,
    cost_tracker: Arc<CostTracker>,
}

impl CostBasedRouter {
    pub async fn route_request(&self, request: &ChatRequest) -> RoutingDecision {
        // è®¡ç®—æ¯ä¸ªæä¾›å•†çš„æˆæœ¬
        let costs = self.calculate_costs(request).await;

        // é€‰æ‹©æˆæœ¬æœ€ä½çš„æä¾›å•†
        let best_provider = costs.iter()
            .min_by(|a, b| a.cost.partial_cmp(&b.cost).unwrap())
            .map(|c| c.provider.clone());

        RoutingDecision {
            provider: best_provider,
            strategy: RoutingStrategy::Cost,
            confidence: 0.9,
        }
    }
}
```

#### åŸºäºæ€§èƒ½çš„è·¯ç”±
```rust
#[derive(Debug)]
pub struct PerformanceBasedRouter {
    providers: Vec<Arc<dyn AiProvider>>,
    metrics: Arc<MetricsCollector>,
}

impl PerformanceBasedRouter {
    pub async fn route_request(&self, request: &ChatRequest) -> RoutingDecision {
        // è·å–æ€§èƒ½æŒ‡æ ‡
        let metrics = self.metrics.get_provider_metrics().await;

        // é€‰æ‹©å“åº”æœ€å¿«çš„æä¾›å•†
        let best_provider = metrics.iter()
            .min_by(|a, b| a.avg_response_time.partial_cmp(&b.avg_response_time).unwrap())
            .map(|m| m.provider.clone());

        RoutingDecision {
            provider: best_provider,
            strategy: RoutingStrategy::Performance,
            confidence: 0.85,
        }
    }
}
```

#### åŸºäºèƒ½åŠ›çš„è·¯ç”±
```rust
#[derive(Debug)]
pub struct CapabilityBasedRouter {
    providers: Vec<Arc<dyn AiProvider>>,
    capabilities: HashMap<String, ProviderCapabilities>,
}

impl CapabilityBasedRouter {
    pub async fn route_request(&self, request: &ChatRequest) -> RoutingDecision {
        // æ£€æŸ¥æ¨¡å‹èƒ½åŠ›è¦æ±‚
        let required_caps = self.extract_capabilities(request);

        // æ‰¾åˆ°æ”¯æŒæ‰€éœ€èƒ½åŠ›çš„æä¾›å•†
        for (provider_id, caps) in &self.capabilities {
            if self.supports_capabilities(caps, &required_caps) {
                return RoutingDecision {
                    provider: Some(provider_id.clone()),
                    strategy: RoutingStrategy::Capability,
                    confidence: 0.95,
                };
            }
        }

        // è¿”å›é»˜è®¤æä¾›å•†
        RoutingDecision {
            provider: self.get_default_provider(),
            strategy: RoutingStrategy::Default,
            confidence: 0.5,
        }
    }
}
```

### ğŸ“Š è·¯ç”±å†³ç­–

#### è·¯ç”±å†³ç­–ç»“æ„
```rust
#[derive(Debug, Clone)]
pub struct RoutingDecision {
    pub provider: Option<String>,
    pub strategy: RoutingStrategy,
    pub confidence: f64,
    pub reasoning: Vec<String>,
    pub alternatives: Vec<String>,
}

#[derive(Debug, Clone)]
pub enum RoutingStrategy {
    Cost,           // æˆæœ¬ä¼˜å…ˆ
    Performance,    // æ€§èƒ½ä¼˜å…ˆ
    Reliability,    // å¯é æ€§ä¼˜å…ˆ
    Capability,     // èƒ½åŠ›åŒ¹é…
    Geographic,     // åœ°ç†ä½ç½®
    LoadBalance,    // è´Ÿè½½å‡è¡¡
    Default,        // é»˜è®¤ç­–ç•¥
}
```

#### å†³ç­–å¼•æ“
```rust
#[derive(Debug)]
pub struct IntelligentRouter {
    routers: Vec<Box<dyn RouterStrategy>>,
    decision_cache: Arc<RwLock<HashMap<String, RoutingDecision>>>,
    metrics: Arc<MetricsCollector>,
}

impl IntelligentRouter {
    pub async fn route(&self, request: &ChatRequest) -> RoutingDecision {
        // ç”Ÿæˆç¼“å­˜é”®
        let cache_key = self.generate_cache_key(request);

        // æ£€æŸ¥ç¼“å­˜
        if let Some(cached) = self.decision_cache.read().await.get(&cache_key) {
            return cached.clone();
        }

        // æ‰§è¡Œè·¯ç”±å†³ç­–
        let mut decisions = Vec::new();
        for router in &self.routers {
            if let Ok(decision) = router.route_request(request).await {
                decisions.push(decision);
            }
        }

        // é€‰æ‹©æœ€ä½³å†³ç­–
        let best_decision = self.select_best_decision(decisions);

        // ç¼“å­˜å†³ç­–
        self.decision_cache.write().await.insert(cache_key, best_decision.clone());

        // è®°å½•æŒ‡æ ‡
        self.metrics.record_routing_decision(&best_decision).await;

        best_decision
    }
}
```

## è´Ÿè½½å‡è¡¡å™¨

### âš–ï¸ è´Ÿè½½å‡è¡¡ç®—æ³•

#### è½®è¯¢ç®—æ³•
```rust
#[derive(Debug)]
pub struct RoundRobinBalancer {
    backends: Vec<Arc<dyn AiProvider>>,
    current_index: AtomicUsize,
}

impl RoundRobinBalancer {
    pub fn select_backend(&self) -> Arc<dyn AiProvider> {
        let index = self.current_index.fetch_add(1, Ordering::Relaxed) % self.backends.len();
        self.backends[index].clone()
    }
}
```

#### åŠ æƒè½®è¯¢ç®—æ³•
```rust
#[derive(Debug)]
pub struct WeightedRoundRobinBalancer {
    backends: Vec<WeightedBackend>,
    total_weight: u32,
    current_weight: AtomicU32,
}

impl WeightedRoundRobinBalancer {
    pub fn select_backend(&self) -> Arc<dyn AiProvider> {
        let mut current = self.current_weight.load(Ordering::Relaxed);

        for backend in &self.backends {
            current = current.wrapping_sub(backend.weight);
            if current < backend.weight {
                self.current_weight.store(current, Ordering::Relaxed);
                return backend.provider.clone();
            }
        }

        // é»˜è®¤è¿”å›ç¬¬ä¸€ä¸ª
        self.backends[0].provider.clone()
    }
}
```

#### æœ€å°‘è¿æ¥ç®—æ³•
```rust
#[derive(Debug)]
pub struct LeastConnectionsBalancer {
    backends: Vec<Arc<dyn AiProvider>>,
    connections: Arc<RwLock<HashMap<String, u32>>>,
}

impl LeastConnectionsBalancer {
    pub async fn select_backend(&self) -> Arc<dyn AiProvider> {
        let connections = self.connections.read().await;

        let best_backend = self.backends.iter()
            .min_by_key(|backend| {
                let provider_id = backend.provider_id();
                *connections.get(provider_id).unwrap_or(&0)
            })
            .cloned();

        best_backend.unwrap_or_else(|| self.backends[0].clone())
    }
}
```

### ğŸ›¡ï¸ ç†”æ–­å™¨æ¨¡å¼

#### ç†”æ–­å™¨å®ç°
```rust
#[derive(Debug)]
pub struct CircuitBreaker {
    state: CircuitBreakerState,
    failure_count: AtomicU32,
    success_count: AtomicU32,
    next_attempt_time: AtomicI64,
    config: CircuitBreakerConfig,
}

#[derive(Debug, Clone, Copy)]
pub enum CircuitBreakerState {
    Closed,      // æ­£å¸¸çŠ¶æ€
    Open,        // ç†”æ–­çŠ¶æ€
    HalfOpen,    // åŠå¼€çŠ¶æ€ï¼Œå…è®¸å°‘é‡è¯·æ±‚æµ‹è¯•
}

impl CircuitBreaker {
    pub async fn call<F, Fut, T>(&self, f: F) -> Result<T, CircuitBreakerError>
    where
        F: FnOnce() -> Fut,
        Fut: Future<Output = Result<T, Box<dyn Error + Send + Sync>>>,
    {
        match self.state {
            CircuitBreakerState::Closed => {
                match f().await {
                    Ok(result) => {
                        self.record_success().await;
                        Ok(result)
                    }
                    Err(e) => {
                        self.record_failure().await;
                        Err(CircuitBreakerError::Wrapped(e))
                    }
                }
            }
            CircuitBreakerState::Open => {
                if self.should_attempt_reset().await {
                    self.set_state(CircuitBreakerState::HalfOpen).await;
                    // å…è®¸ä¸€æ¬¡å°è¯•
                    match f().await {
                        Ok(result) => {
                            self.record_success().await;
                            Ok(result)
                        }
                        Err(e) => {
                            self.record_failure().await;
                            Err(CircuitBreakerError::Wrapped(e))
                        }
                    }
                } else {
                    Err(CircuitBreakerError::Open)
                }
            }
            CircuitBreakerState::HalfOpen => {
                // åªå…è®¸ä¸€ä¸ªè¯·æ±‚é€šè¿‡
                match f().await {
                    Ok(result) => {
                        self.record_success().await;
                        Ok(result)
                    }
                    Err(e) => {
                        self.record_failure().await;
                        Err(CircuitBreakerError::Wrapped(e))
                    }
                }
            }
        }
    }
}
```

## AIå®¢æˆ·ç«¯æ¥å£

### ğŸ¯ ç»Ÿä¸€å®¢æˆ·ç«¯

#### å®¢æˆ·ç«¯è®¾è®¡
```rust
#[derive(Clone)]
pub struct AiBackendClient {
    providers: HashMap<String, Arc<dyn AiProvider>>,
    router: Arc<IntelligentRouter>,
    load_balancer: Arc<dyn LoadBalancer>,
    circuit_breaker: Arc<CircuitBreaker>,
    cache: Arc<Cache>,
    metrics: Arc<MetricsCollector>,
}

impl AiBackendClient {
    /// åˆ›å»ºæ–°çš„AIåç«¯å®¢æˆ·ç«¯
    pub fn new() -> Self {
        AiBackendClient {
            providers: HashMap::new(),
            router: Arc::new(IntelligentRouter::new()),
            load_balancer: Arc::new(RoundRobinBalancer::new()),
            circuit_breaker: Arc::new(CircuitBreaker::new(Default::default())),
            cache: Arc::new(Cache::new()),
            metrics: Arc::new(MetricsCollector::new()),
        }
    }

    /// æ³¨å†ŒAIæä¾›å•†
    pub fn register_provider(&mut self, provider: Arc<dyn AiProvider>) -> Result<(), AiError> {
        let provider_id = provider.provider_id();
        self.providers.insert(provider_id.to_string(), provider);
        Ok(())
    }

    /// æ–‡æœ¬å¯¹è¯
    pub async fn chat_completion(&self, request: ChatRequest) -> Result<ChatResponse, AiError> {
        // æ£€æŸ¥ç¼“å­˜
        if let Some(cached) = self.cache.get(&request).await? {
            self.metrics.record_cache_hit();
            return Ok(cached);
        }

        // è·¯ç”±å†³ç­–
        let decision = self.router.route(&request).await;

        // è·å–æä¾›å•†
        let provider = self.get_provider(&decision.provider)?;

        // ç†”æ–­å™¨ä¿æŠ¤
        let response = self.circuit_breaker.call(|| async {
            provider.chat_completion(request.clone()).await
        }).await?;

        // ç¼“å­˜ç»“æœ
        self.cache.set(request, response.clone()).await?;

        // è®°å½•æŒ‡æ ‡
        self.metrics.record_request(&request, &response).await;

        Ok(response)
    }
}
```

#### æµå¼å“åº”æ”¯æŒ
```rust
impl AiBackendClient {
    /// æµå¼æ–‡æœ¬å¯¹è¯
    pub async fn chat_completion_stream(
        &self,
        request: ChatRequest,
    ) -> Result<Pin<Box<dyn Stream<Item = Result<ChatResponse, AiError>> + Send>>, AiError> {
        // è·¯ç”±å†³ç­–
        let decision = self.router.route(&request).await;
        let provider = self.get_provider(&decision.provider)?;

        // è·å–æµå¼å“åº”
        let stream = provider.chat_completion_stream(request).await?;

        // æ·»åŠ æŒ‡æ ‡æ”¶é›†çš„åŒ…è£…å™¨
        let metrics = self.metrics.clone();
        let request_clone = request.clone();

        let instrumented_stream = stream.map(move |result| {
            match &result {
                Ok(response) => {
                    // å¼‚æ­¥è®°å½•æŒ‡æ ‡ï¼ˆè¿™é‡Œéœ€è¦å°å¿ƒå¤„ç†ï¼‰
                    tokio::spawn(async move {
                        metrics.record_stream_chunk(&request_clone, response).await;
                    });
                }
                Err(_) => {}
            }
            result
        });

        Ok(Box::pin(instrumented_stream))
    }
}
```

## ç›‘æ§å’ŒæŒ‡æ ‡

### ğŸ“Š æ€§èƒ½æŒ‡æ ‡

#### è¯·æ±‚æŒ‡æ ‡
```rust
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RequestMetrics {
    pub total_requests: u64,
    pub successful_requests: u64,
    pub failed_requests: u64,
    pub avg_response_time: f64,
    pub p95_response_time: f64,
    pub p99_response_time: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProviderMetrics {
    pub provider_id: String,
    pub request_count: u64,
    pub error_count: u64,
    pub avg_latency: f64,
    pub success_rate: f64,
    pub cost_per_request: f64,
}
```

#### æŒ‡æ ‡æ”¶é›†å™¨
```rust
pub struct MetricsCollector {
    request_metrics: Arc<RwLock<HashMap<String, RequestMetrics>>>,
    provider_metrics: Arc<RwLock<HashMap<String, ProviderMetrics>>>,
    histogram: Arc<RwLock<Histogram>>,
}

impl MetricsCollector {
    pub async fn record_request(&self, request: &ChatRequest, response: &ChatResponse) {
        // è®°å½•è¯·æ±‚æŒ‡æ ‡
        let mut metrics = self.request_metrics.write().await;
        let key = format!("{}_{}", request.model, response.usage.map(|u| u.model).unwrap_or_default());

        let entry = metrics.entry(key).or_insert(RequestMetrics::default());
        entry.total_requests += 1;
        entry.successful_requests += 1;

        // è®°å½•å»¶è¿Ÿ
        if let Some(latency) = response.usage.as_ref().and_then(|u| u.total_latency) {
            self.histogram.write().await.record(latency);
        }
    }
}
```

## é…ç½®ç®¡ç†

### ğŸ› ï¸ æä¾›å•†é…ç½®

#### é…ç½®ç»“æ„
```toml
[ai.providers.openai]
api_key = "sk-..."
base_url = "https://api.openai.com/v1"
timeout = 30
max_retries = 3
models = ["gpt-4", "gpt-3.5-turbo"]

[ai.providers.anthropic]
api_key = "sk-ant-..."
base_url = "https://api.anthropic.com"
version = "2023-06-01"
timeout = 60
max_retries = 2
models = ["claude-3-opus", "claude-3-sonnet"]

[ai.routing]
strategy = "intelligent"
cache_ttl = 300
fallback_provider = "openai"

[ai.load_balancing]
algorithm = "weighted_round_robin"
health_check_interval = 30

[ai.circuit_breaker]
failure_threshold = 5
recovery_timeout = 60
success_threshold = 3
```

#### åŠ¨æ€é…ç½®
```rust
impl AiBackendClient {
    /// çƒ­æ›´æ–°é…ç½®
    pub async fn update_config(&mut self, config: AiConfig) -> Result<(), AiError> {
        // æ›´æ–°è·¯ç”±ç­–ç•¥
        self.router.update_config(&config.routing).await?;

        // æ›´æ–°è´Ÿè½½å‡è¡¡
        self.load_balancer.update_config(&config.load_balancing).await?;

        // æ›´æ–°ç†”æ–­å™¨
        self.circuit_breaker.update_config(&config.circuit_breaker).await?;

        Ok(())
    }
}
```

## é”™è¯¯å¤„ç†

### ğŸ­ é”™è¯¯ç±»å‹

#### AIé”™è¯¯å®šä¹‰
```rust
#[derive(Error, Debug)]
pub enum AiError {
    #[error("Provider error: {provider} - {message}")]
    ProviderError {
        provider: String,
        message: String,
        source: Option<Box<dyn Error + Send + Sync>>,
    },

    #[error("Rate limit exceeded for provider: {provider}")]
    RateLimitExceeded {
        provider: String,
        retry_after: Option<u32>,
    },

    #[error("Invalid request: {message}")]
    InvalidRequest {
        message: String,
    },

    #[error("Network error: {message}")]
    NetworkError {
        message: String,
        source: reqwest::Error,
    },

    #[error("Authentication failed for provider: {provider}")]
    AuthenticationError {
        provider: String,
    },

    #[error("Quota exceeded for provider: {provider}")]
    QuotaExceeded {
        provider: String,
    },

    #[error("Circuit breaker open for provider: {provider}")]
    CircuitBreakerOpen {
        provider: String,
    },

    #[error("Timeout error: {message}")]
    TimeoutError {
        message: String,
    },
}
```

#### é”™è¯¯æ¢å¤ç­–ç•¥
```rust
impl AiBackendClient {
    async fn handle_error(&self, error: AiError, request: &ChatRequest) -> Result<ChatResponse, AiError> {
        match error {
            AiError::RateLimitExceeded { provider, retry_after } => {
                // å®ç°é‡è¯•é€»è¾‘
                if let Some(delay) = retry_after {
                    tokio::time::sleep(Duration::from_secs(delay as u64)).await;
                    return self.chat_completion(request.clone()).await;
                }
                Err(error)
            }

            AiError::CircuitBreakerOpen { .. } => {
                // å°è¯•å…¶ä»–æä¾›å•†
                self.try_fallback_provider(request).await
            }

            AiError::ProviderError { .. } => {
                // è®°å½•é”™è¯¯å¹¶å°è¯•é™çº§
                self.metrics.record_error(&error).await;
                self.try_degraded_mode(request).await
            }

            _ => Err(error),
        }
    }
}
```

## æµ‹è¯•å’ŒéªŒè¯

### ğŸ§ª å•å…ƒæµ‹è¯•

#### æä¾›å•†æµ‹è¯•
```rust
#[cfg(test)]
mod tests {
    use super::*;
    use mockito::Server;

    #[tokio::test]
    async fn test_openai_chat_completion() {
        let mut server = Server::new_async().await;

        // Mock OpenAI API
        let mock = server.mock("POST", "/v1/chat/completions")
            .with_status(200)
            .with_body(r#"{
                "id": "chatcmpl-123",
                "object": "chat.completion",
                "created": 1677652288,
                "model": "gpt-3.5-turbo",
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": "Hello! How can I help you?"
                    },
                    "finish_reason": "stop"
                }],
                "usage": {
                    "prompt_tokens": 13,
                    "completion_tokens": 7,
                    "total_tokens": 20
                }
            }"#)
            .create();

        let config = OpenAIConfig {
            api_key: "test-key".to_string(),
            base_url: server.url(),
            ..Default::default()
        };

        let provider = OpenAIProvider::new(config);
        let request = ChatRequest {
            model: "gpt-3.5-turbo".to_string(),
            messages: vec![ChatMessage {
                role: MessageRole::User,
                content: MessageContent::Text("Hello!".to_string()),
                name: None,
            }],
            ..Default::default()
        };

        let response = provider.chat_completion(request).await.unwrap();

        assert_eq!(response.choices[0].message.content, MessageContent::Text("Hello! How can I help you?".to_string()));
        mock.assert();
    }
}
```

#### é›†æˆæµ‹è¯•
```rust
#[tokio::test]
async fn test_routing_integration() {
    let client = AiBackendClient::new();

    // æ³¨å†Œæä¾›å•†
    let openai = Arc::new(OpenAIProvider::new(openai_config));
    let anthropic = Arc::new(AnthropicProvider::new(anthropic_config));

    client.register_provider(openai).await.unwrap();
    client.register_provider(anthropic).await.unwrap();

    // æµ‹è¯•è·¯ç”±
    let request = ChatRequest {
        model: "gpt-4".to_string(),
        messages: vec![ChatMessage {
            role: MessageRole::User,
            content: MessageContent::Text("Test message".to_string()),
            name: None,
        }],
        ..Default::default()
    };

    let response = client.chat_completion(request).await.unwrap();
    assert!(!response.choices.is_empty());
}
```

### ğŸ“Š æ€§èƒ½æµ‹è¯•

#### è´Ÿè½½æµ‹è¯•
```rust
#[tokio::test]
async fn benchmark_concurrent_requests() {
    let client = AiBackendClient::new();
    // æ³¨å†Œæä¾›å•†...

    let request = ChatRequest {
        model: "gpt-3.5-turbo".to_string(),
        messages: vec![ChatMessage {
            role: MessageRole::User,
            content: MessageContent::Text("Hello".to_string()),
            name: None,
        }],
        ..Default::default()
    };

    // å¹¶å‘æµ‹è¯•
    let handles: Vec<_> = (0..100).map(|_| {
        let client_clone = client.clone();
        let request_clone = request.clone();
        tokio::spawn(async move {
            client_clone.chat_completion(request_clone).await
        })
    }).collect();

    // æ”¶é›†ç»“æœ
    let results = futures::future::join_all(handles).await;
    let success_count = results.iter()
        .filter(|r| r.as_ref().unwrap().as_ref().unwrap().is_ok())
        .count();

    assert_eq!(success_count, 100);
}
```

## éƒ¨ç½²å’Œè¿ç»´

### ğŸ³ å®¹å™¨åŒ–éƒ¨ç½²

#### Dockerfile
```dockerfile
FROM rust:1.70-slim as builder
WORKDIR /app
COPY . .
RUN cargo build --release --bin sira-ai-backends

FROM debian:bookworm-slim
RUN apt-get update && apt-get install -y ca-certificates
COPY --from=builder /app/target/release/sira-ai-backends /usr/local/bin/
EXPOSE 9090
CMD ["sira-ai-backends"]
```

#### Docker Compose
```yaml
version: '3.8'
services:
  ai-backends:
    build: .
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
    ports:
      - "9090:9090"
    volumes:
      - ./config:/app/config
    restart: unless-stopped
```

### ğŸ“Š ç›‘æ§å‘Šè­¦

#### PrometheusæŒ‡æ ‡
```yaml
# AIè¯·æ±‚æŒ‡æ ‡
ai_requests_total{provider="openai", model="gpt-4"} 1500

# AIå“åº”æ—¶é—´
ai_request_duration_seconds{provider="openai", quantile="0.95"} 2.5

# AIé”™è¯¯ç‡
ai_error_rate{provider="openai"} 0.02

# AIæˆæœ¬è·Ÿè¸ª
ai_cost_total{provider="openai", currency="USD"} 25.50
```

#### Grafanaä»ªè¡¨æ¿
- AIè¯·æ±‚é‡è¶‹åŠ¿å›¾
- å„æä¾›å•†å“åº”æ—¶é—´å¯¹æ¯”
- é”™è¯¯ç‡ç›‘æ§é¢æ¿
- æˆæœ¬ä½¿ç”¨åˆ†æå›¾
- æ¨¡å‹ä½¿ç”¨åˆ†å¸ƒç»Ÿè®¡

## å®‰å…¨è€ƒè™‘

### ğŸ” APIå¯†é’¥ç®¡ç†
- å¯†é’¥åŠ å¯†å­˜å‚¨
- å¯†é’¥è½®æ¢æœºåˆ¶
- å¯†é’¥è®¿é—®å®¡è®¡
- å¯†é’¥æ³„éœ²æ£€æµ‹

### ğŸ›¡ï¸ è¯·æ±‚å®‰å…¨
- è¾“å…¥éªŒè¯å’Œæ¸…ç†
- æ•æ„Ÿä¿¡æ¯è¿‡æ»¤
- è¯·æ±‚é¢‘ç‡é™åˆ¶
- å¼‚å¸¸æ£€æµ‹å’Œé˜»æ–­

### ğŸ“Š å®¡è®¡æ—¥å¿—
- å®Œæ•´çš„APIè°ƒç”¨è®°å½•
- ç”¨æˆ·è¡Œä¸ºåˆ†æ
- å®‰å…¨äº‹ä»¶è¿½è¸ª
- åˆè§„æ€§æŠ¥å‘Šç”Ÿæˆ

## æ‰©å±•æœºåˆ¶

### â• æ·»åŠ æ–°AIæä¾›å•†

#### å®ç°æä¾›å•†æ¥å£
```rust
#[async_trait]
impl AiProvider for CustomProvider {
    fn provider_id(&self) -> &str {
        "custom"
    }

    async fn chat_completion(&self, request: ChatRequest) -> Result<ChatResponse, AiError> {
        // å®ç°è‡ªå®šä¹‰æä¾›å•†çš„å¯¹è¯é€»è¾‘
        unimplemented!()
    }

    async fn health_check(&self) -> Result<(), AiError> {
        // å®ç°å¥åº·æ£€æŸ¥
        Ok(())
    }
}
```

#### æ³¨å†Œæ–°æä¾›å•†
```rust
let custom_provider = Arc::new(CustomProvider::new(config));
client.register_provider(custom_provider).await?;
```

### ğŸ¯ è‡ªå®šä¹‰è·¯ç”±ç­–ç•¥

#### å®ç°è·¯ç”±ç­–ç•¥
```rust
#[async_trait]
impl RouterStrategy for CustomRouter {
    async fn route_request(&self, request: &ChatRequest) -> Result<RoutingDecision, AiError> {
        // å®ç°è‡ªå®šä¹‰è·¯ç”±é€»è¾‘
        let decision = RoutingDecision {
            provider: Some("custom-provider".to_string()),
            strategy: RoutingStrategy::Custom,
            confidence: 0.8,
        };
        Ok(decision)
    }
}
```

#### æ³¨å†Œè·¯ç”±ç­–ç•¥
```rust
let custom_router = Arc::new(CustomRouter::new());
client.router.add_strategy(custom_router).await?;
```

## æœªæ¥è§„åˆ’

### ğŸš€ å¢å¼ºåŠŸèƒ½
- [ ] æ”¯æŒæ›´å¤šAIæä¾›å•† (Cohere, AI21, etc.)
- [ ] å®ç°æ¨¡å‹å¾®è°ƒAPIé›†æˆ
- [ ] æ·»åŠ æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•
- [ ] æ”¯æŒè‡ªå®šä¹‰æ¨¡å‹éƒ¨ç½²
- [ ] å®ç°æ¨¡å‹A/Bæµ‹è¯•

### âš¡ æ€§èƒ½ä¼˜åŒ–
- [ ] å®ç°è¯·æ±‚æ‰¹å¤„ç†
- [ ] æ·»åŠ æ™ºèƒ½ç¼“å­˜ç­–ç•¥
- [ ] ä¼˜åŒ–åºåˆ—åŒ–æ€§èƒ½
- [ ] æ”¯æŒHTTP/2è¿æ¥å¤ç”¨
- [ ] å®ç°è¿æ¥æ± é¢„çƒ­

### ğŸ¤– AIå¢å¼º
- [ ] AIé©±åŠ¨çš„è·¯ç”±ä¼˜åŒ–
- [ ] æ™ºèƒ½ç¼“å­˜é¢„å–
- [ ] è‡ªåŠ¨æ•…éšœé¢„æµ‹
- [ ] è‡ªé€‚åº”è´Ÿè½½å‡è¡¡
- [ ] æˆæœ¬ä¼˜åŒ–å»ºè®®

---

**Sira AI Backends** - è¿æ¥AIä¸–ç•Œçš„æ¡¥æ¢
