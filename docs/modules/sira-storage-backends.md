# Sira Storage Backends - å­˜å‚¨ä¼˜åŒ–æ‰©å±• (Storage Optimization Extension)

## æ¦‚è¿°

Sira Storage Backends æ˜¯**æ™ºèƒ½ç½‘å…³çš„å­˜å‚¨ä¼˜åŒ–æ‰©å±•**ï¼Œä¸ºç½‘å…³æä¾›é«˜æ€§èƒ½çš„å­˜å‚¨åç«¯å’Œæ•°æ®ç®¡ç†èƒ½åŠ›ã€‚å½“ç½‘å…³éœ€è¦å¤„ç†å¤§é‡æ•°æ®ã€é«˜å¹¶å‘è®¿é—®æˆ–ç‰¹æ®Šçš„å­˜å‚¨ä¼˜åŒ–éœ€æ±‚æ—¶ï¼Œå¯ä»¥é€‰æ‹©å¯ç”¨Storageæ¨¡å—ã€‚

**åœ¨æ™ºèƒ½ç½‘å…³ç”Ÿæ€ä¸­çš„å®šä½**ï¼šä½œä¸ºå¯é€‰çš„å­˜å‚¨å¢å¼ºå·¥å…·åŒ…ï¼Œç½‘å…³æ ¸å¿ƒå·²ç»å…·å¤‡åŸºç¡€çš„å­˜å‚¨èƒ½åŠ›ï¼ŒStorageæä¾›ä¸“ä¸šçº§çš„å¤šåç«¯å­˜å‚¨å’Œæ€§èƒ½ä¼˜åŒ–åŠŸèƒ½ã€‚

**AOSå“²å­¦ä½“ç°**ï¼š
- **å¼ é‡åŸç”Ÿå­˜å‚¨**ï¼šæ”¯æŒä»»æ„ç»´åº¦å¼ é‡çš„ç›´æ¥å­˜å‚¨
- **æ™ºèƒ½å‹ç¼©ä¼˜åŒ–**ï¼šåŸºäºNVIDIAå’ŒTsinghuaç ”ç©¶çš„å‹ç¼©æŠ€æœ¯
- **è‡ªé€‚åº”å­˜å‚¨ç­–ç•¥**ï¼šæ ¹æ®è®¿é—®æ¨¡å¼åŠ¨æ€è°ƒæ•´å­˜å‚¨ç­–ç•¥

## AOSæŠ€æœ¯æ ˆæ˜ å°„

### ğŸ¯ å¯¹åº”æŠ€æœ¯é¢†åŸŸ
**AIç¤¾ä¼šçš„"ç‰©ç†æ³•åˆ™"â€”â€”åº•å±‚åè®®ä¸é€šä¿¡ (å­˜å‚¨ä¼˜åŒ–æ–¹å‘)**

### ğŸ”§ æ ¸å¿ƒæŠ€æœ¯æ ˆ

#### å¼ é‡åŸç”Ÿå­˜å‚¨åè®® (Tensor-Native Storage Protocol)
- **åºåˆ—åŒ–ä¼˜åŒ–**: FlatBuffers, Apache Avro ç”¨äºé«˜æ•ˆå¼ é‡åºåˆ—åŒ–
- **é›¶æ‹·è´ä¼ è¾“**: Apache Arrow Flight æ”¯æŒå¤§è§„æ¨¡å¼ é‡æ•°æ®ä¼ è¾“
- **åŸç”Ÿå¼ é‡æ ¼å¼**: æ”¯æŒä»»æ„ç»´åº¦å¼ é‡çš„ç›´æ¥å­˜å‚¨ï¼Œæ— éœ€è½¬æ¢

#### æ™ºèƒ½å­˜å‚¨å‹ç¼©ä¸ä¼˜åŒ– (Intelligent Storage Compression)
- **KV Cacheå‹ç¼©**: NVIDIAç ”ç©¶æ–¹å‘ï¼Œå‹ç¼©LLMæ¨ç†æ—¶çš„ä¸Šä¸‹æ–‡å­˜å‚¨
- **æ³¨æ„åŠ›å­˜å‚¨ä¼˜åŒ–**: INFLLM-V2å¯å‘çš„è‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶å­˜å‚¨
- **è§†è§‰tokenå‹ç¼©**: Vision-centric Token Compressionç ”ç©¶åº”ç”¨

#### è‡ªé€‚åº”å­˜å‚¨ç­–ç•¥ (Adaptive Storage Strategy)
- **è®¿é—®æ¨¡å¼å­¦ä¹ **: ä»å†å²è®¿é—®æ¨¡å¼ä¸­å­¦ä¹ æœ€ä¼˜å­˜å‚¨ç­–ç•¥
- **å¤šåç«¯æ™ºèƒ½è·¯ç”±**: æ ¹æ®æ•°æ®ç‰¹å¾å’Œè®¿é—®æ¨¡å¼é€‰æ‹©æœ€ä½³å­˜å‚¨åç«¯
- **æ€§èƒ½ç›‘æ§ä¼˜åŒ–**: å®æ—¶ç›‘æ§å¹¶è°ƒæ•´å­˜å‚¨ç­–ç•¥ä»¥ä¼˜åŒ–æ€§èƒ½

#### ç›¸å…³ç ”ç©¶è®ºæ–‡
- **KV Cacheå‹ç¼©ç›¸å…³ç ”ç©¶** (NVIDIA)
- **"INFLLM-V2: Dense-Sparse Switchable Attention"** (Tsinghua, OpenBMB)
- **"Vision-centric Token Compression in Large Language Model"** (Nanjing University)

## æ ¸å¿ƒç»„ä»¶

### ğŸ’¾ å­˜å‚¨æŠ½è±¡æ¥å£ (Storage Abstraction Interface)

#### ç»Ÿä¸€å­˜å‚¨æ¥å£
```rust
#[async_trait]
pub trait StorageBackend: Send + Sync {
    /// å­˜å‚¨æ•°æ®
    async fn put(&self, key: &str, value: &[u8], options: StorageOptions) -> Result<(), StorageError>;

    /// æ£€ç´¢æ•°æ®
    async fn get(&self, key: &str) -> Result<Option<Vec<u8>>, StorageError>;

    /// åˆ é™¤æ•°æ®
    async fn delete(&self, key: &str) -> Result<bool, StorageError>;

    /// æ£€æŸ¥é”®æ˜¯å¦å­˜åœ¨
    async fn exists(&self, key: &str) -> Result<bool, StorageError>;

    /// åˆ—å‡ºé”®
    async fn list_keys(&self, prefix: Option<&str>, limit: Option<usize>) -> Result<Vec<String>, StorageError>;

    /// è·å–å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
    async fn stats(&self) -> Result<StorageStats, StorageError>;

    /// æ‰¹é‡æ“ä½œ
    async fn batch(&self, operations: Vec<BatchOperation>) -> Result<Vec<BatchResult>, StorageError>;

    /// å¥åº·æ£€æŸ¥
    async fn health_check(&self) -> Result<HealthStatus, StorageError>;
}

#[derive(Debug, Clone)]
pub struct StorageOptions {
    pub ttl: Option<Duration>,
    pub compression: Option<CompressionType>,
    pub encryption: Option<EncryptionType>,
    pub consistency: ConsistencyLevel,
}

#[derive(Debug, Clone, Copy)]
pub enum ConsistencyLevel {
    Strong,     // å¼ºä¸€è‡´æ€§
    Eventual,   // æœ€ç»ˆä¸€è‡´æ€§
    Weak,       // å¼±ä¸€è‡´æ€§
}
```

#### å­˜å‚¨å®¢æˆ·ç«¯
```rust
#[derive(Clone)]
pub struct StorageClient {
    backends: Arc<RwLock<HashMap<String, Arc<dyn StorageBackend>>>>,
    router: Arc<StorageRouter>,
    metrics: Arc<MetricsCollector>,
    cache: Arc<Cache>,
}

impl StorageClient {
    /// åˆ›å»ºå­˜å‚¨å®¢æˆ·ç«¯
    pub fn new() -> Self {
        StorageClient {
            backends: Arc::new(RwLock::new(HashMap::new())),
            router: Arc::new(StorageRouter::new()),
            metrics: Arc::new(MetricsCollector::new()),
            cache: Arc::new(Cache::new()),
        }
    }

    /// æ³¨å†Œå­˜å‚¨åç«¯
    pub async fn register_backend(&self, name: &str, backend: Arc<dyn StorageBackend>) -> Result<(), StorageError> {
        let mut backends = self.backends.write().await;
        backends.insert(name.to_string(), backend);
        Ok(())
    }

    /// æ™ºèƒ½å­˜å‚¨æ•°æ®
    pub async fn store(&self, key: &str, value: &[u8], options: StorageOptions) -> Result<(), StorageError> {
        // è·¯ç”±é€‰æ‹©åç«¯
        let backend_name = self.router.select_backend(key, &options).await?;
        let backend = self.get_backend(&backend_name).await?;

        // å­˜å‚¨æ•°æ®
        backend.put(key, value, options.clone()).await?;

        // æ›´æ–°ç¼“å­˜
        if options.ttl.is_some() {
            self.cache.set(key, value, options.ttl).await?;
        }

        // è®°å½•æŒ‡æ ‡
        self.metrics.record_operation("store", &backend_name, true).await?;

        Ok(())
    }

    /// æ™ºèƒ½æ£€ç´¢æ•°æ®
    pub async fn retrieve(&self, key: &str) -> Result<Option<Vec<u8>>, StorageError> {
        // æ£€æŸ¥ç¼“å­˜
        if let Some(data) = self.cache.get(key).await? {
            self.metrics.record_cache_hit().await?;
            return Ok(Some(data));
        }

        // è·¯ç”±é€‰æ‹©åç«¯
        let backend_name = self.router.select_backend(key, &StorageOptions::default()).await?;
        let backend = self.get_backend(&backend_name).await?;

        // æ£€ç´¢æ•°æ®
        let result = backend.get(key).await?;

        // æ›´æ–°ç¼“å­˜
        if let Some(ref data) = result {
            self.cache.set(key, data, None).await?;
        }

        // è®°å½•æŒ‡æ ‡
        self.metrics.record_operation("retrieve", &backend_name, result.is_some()).await?;

        Ok(result)
    }
}
```

### ğŸ¯ æ™ºèƒ½è·¯ç”±å™¨ (Intelligent Router)

#### å­˜å‚¨ç­–ç•¥
```rust
#[derive(Debug)]
pub struct StorageRouter {
    strategies: Vec<Box<dyn RoutingStrategy>>,
    backend_stats: Arc<RwLock<HashMap<String, BackendStats>>>,
    data_patterns: Arc<RwLock<HashMap<String, DataPattern>>>,
}

#[async_trait]
pub trait RoutingStrategy: Send + Sync {
    async fn select_backend(&self, key: &str, options: &StorageOptions, stats: &HashMap<String, BackendStats>) -> Result<String, RouterError>;
}

impl StorageRouter {
    /// é€‰æ‹©å­˜å‚¨åç«¯
    pub async fn select_backend(&self, key: &str, options: &StorageOptions) -> Result<String, RouterError> {
        let stats = self.backend_stats.read().await.clone();

        // åº”ç”¨è·¯ç”±ç­–ç•¥
        for strategy in &self.strategies {
            if let Ok(backend) = strategy.select_backend(key, options, &stats).await {
                return Ok(backend);
            }
        }

        // é»˜è®¤ç­–ç•¥
        Ok("default".to_string())
    }

    /// åŸºäºæ€§èƒ½çš„è·¯ç”±
    pub async fn performance_based_routing(&self, key: &str, _options: &StorageOptions, stats: &HashMap<String, BackendStats>) -> Result<String, RouterError> {
        // é€‰æ‹©å“åº”æœ€å¿«çš„åç«¯
        let best_backend = stats.iter()
            .min_by(|a, b| a.1.avg_response_time.partial_cmp(&b.1.avg_response_time).unwrap())
            .map(|(name, _)| name.clone())
            .ok_or(RouterError::NoBackendAvailable)?;

        Ok(best_backend)
    }

    /// åŸºäºæˆæœ¬çš„è·¯ç”±
    pub async fn cost_based_routing(&self, key: &str, _options: &StorageOptions, stats: &HashMap<String, BackendStats>) -> Result<String, RouterError> {
        // é€‰æ‹©æˆæœ¬æœ€ä½çš„åç«¯
        let best_backend = stats.iter()
            .min_by(|a, b| a.1.cost_per_operation.partial_cmp(&b.1.cost_per_operation).unwrap())
            .map(|(name, _)| name.clone())
            .ok_or(RouterError::NoBackendAvailable)?;

        Ok(best_backend)
    }

    /// åŸºäºæ•°æ®ç±»å‹çš„è·¯ç”±
    pub async fn data_type_based_routing(&self, key: &str, _options: &StorageOptions, _stats: &HashMap<String, BackendStats>) -> Result<String, RouterError> {
        let patterns = self.data_patterns.read().await;

        // æ ¹æ®é”®æ¨¡å¼é€‰æ‹©åç«¯
        for (pattern, data_pattern) in patterns.iter() {
            if key.contains(pattern) {
                return Ok(data_pattern.preferred_backend.clone());
            }
        }

        Ok("default".to_string())
    }
}
```

### ğŸ’¾ å­˜å‚¨åç«¯å®ç° (Storage Backend Implementations)

#### å†…å­˜å­˜å‚¨
```rust
pub struct MemoryBackend {
    data: Arc<RwLock<HashMap<String, StorageItem>>>,
    max_size: usize,
    current_size: Arc<AtomicUsize>,
}

impl MemoryBackend {
    pub fn new(max_size: usize) -> Self {
        MemoryBackend {
            data: Arc::new(RwLock::new(HashMap::new())),
            max_size,
            current_size: Arc::new(AtomicUsize::new(0)),
        }
    }
}

#[async_trait]
impl StorageBackend for MemoryBackend {
    async fn put(&self, key: &str, value: &[u8], options: StorageOptions) -> Result<(), StorageError> {
        let item = StorageItem {
            data: value.to_vec(),
            created_at: Utc::now(),
            ttl: options.ttl,
            compressed: options.compression.is_some(),
            encrypted: options.encryption.is_some(),
        };

        let item_size = item.data.len();
        let new_total_size = self.current_size.load(Ordering::Relaxed) + item_size;

        // æ£€æŸ¥å®¹é‡é™åˆ¶
        if new_total_size > self.max_size {
            return Err(StorageError::OutOfSpace);
        }

        let mut data = self.data.write().await;
        data.insert(key.to_string(), item);
        self.current_size.store(new_total_size, Ordering::Relaxed);

        Ok(())
    }

    async fn get(&self, key: &str) -> Result<Option<Vec<u8>>, StorageError> {
        let data = self.data.read().await;

        if let Some(item) = data.get(key) {
            // æ£€æŸ¥TTL
            if let Some(ttl) = item.ttl {
                if Utc::now() > item.created_at + ttl {
                    return Ok(None);
                }
            }

            Ok(Some(item.data.clone()))
        } else {
            Ok(None)
        }
    }
}
```

#### Rediså­˜å‚¨
```rust
pub struct RedisBackend {
    client: redis::Client,
    prefix: String,
    connection_manager: Arc<ConnectionManager>,
}

impl RedisBackend {
    pub fn new(redis_url: &str, prefix: String) -> Result<Self, StorageError> {
        let client = redis::Client::open(redis_url)?;
        let connection_manager = Arc::new(ConnectionManager::new(client.clone()));

        Ok(RedisBackend {
            client,
            prefix,
            connection_manager,
        })
    }
}

#[async_trait]
impl StorageBackend for RedisBackend {
    async fn put(&self, key: &str, value: &[u8], options: StorageOptions) -> Result<(), StorageError> {
        let mut conn = self.connection_manager.get_connection().await?;
        let full_key = format!("{}:{}", self.prefix, key);

        // å¤„ç†å‹ç¼©
        let data = if let Some(compression) = options.compression {
            self.compress_data(value, compression).await?
        } else {
            value.to_vec()
        };

        // å¤„ç†åŠ å¯†
        let final_data = if let Some(encryption) = options.encryption {
            self.encrypt_data(&data, encryption).await?
        } else {
            data
        };

        // è®¾ç½®TTL
        if let Some(ttl) = options.ttl {
            redis::cmd("SETEX")
                .arg(&full_key)
                .arg(ttl.as_secs())
                .arg(final_data)
                .query_async(&mut conn)
                .await?;
        } else {
            redis::cmd("SET")
                .arg(&full_key)
                .arg(final_data)
                .query_async(&mut conn)
                .await?;
        }

        Ok(())
    }

    async fn get(&self, key: &str) -> Result<Option<Vec<u8>>, StorageError> {
        let mut conn = self.connection_manager.get_connection().await?;
        let full_key = format!("{}:{}", self.prefix, key);

        let data: Option<Vec<u8>> = redis::cmd("GET")
            .arg(&full_key)
            .query_async(&mut conn)
            .await?;

        if let Some(mut data) = data {
            // å¤„ç†è§£å¯†
            data = self.decrypt_data(&data).await?;

            // å¤„ç†è§£å‹ç¼©
            data = self.decompress_data(&data).await?;

            Ok(Some(data))
        } else {
            Ok(None)
        }
    }
}
```

#### PostgreSQLå­˜å‚¨
```rust
pub struct PostgresBackend {
    pool: sqlx::PgPool,
    table_name: String,
}

impl PostgresBackend {
    pub fn new(database_url: &str, table_name: String) -> Result<Self, StorageError> {
        // åˆ›å»ºè¿æ¥æ± 
        // å®ç°è¡¨åˆ›å»ºå’Œè¿ç§»é€»è¾‘
        unimplemented!()
    }
}

#[async_trait]
impl StorageBackend for PostgresBackend {
    async fn put(&self, key: &str, value: &[u8], options: StorageOptions) -> Result<(), StorageError> {
        let expires_at = options.ttl.map(|ttl| Utc::now() + ttl);

        sqlx::query(&format!(
            "INSERT INTO {} (key, value, expires_at, compressed, encrypted, created_at)
             VALUES ($1, $2, $3, $4, $5, $6)
             ON CONFLICT (key) DO UPDATE SET
               value = EXCLUDED.value,
               expires_at = EXCLUDED.expires_at,
               compressed = EXCLUDED.compressed,
               encrypted = EXCLUDED.encrypted",
            self.table_name
        ))
        .bind(key)
        .bind(value)
        .bind(expires_at)
        .bind(options.compression.is_some())
        .bind(options.encryption.is_some())
        .bind(Utc::now())
        .execute(&self.pool)
        .await?;

        Ok(())
    }

    async fn get(&self, key: &str) -> Result<Option<Vec<u8>>, StorageError> {
        let record: Option<(Vec<u8>, Option<DateTime<Utc>>)> = sqlx::query_as(&format!(
            "SELECT value, expires_at FROM {} WHERE key = $1",
            self.table_name
        ))
        .bind(key)
        .fetch_optional(&self.pool)
        .await?;

        if let Some((value, expires_at)) = record {
            // æ£€æŸ¥è¿‡æœŸ
            if let Some(expires) = expires_at {
                if Utc::now() > expires {
                    // åˆ é™¤è¿‡æœŸæ•°æ®
                    sqlx::query(&format!("DELETE FROM {} WHERE key = $1", self.table_name))
                        .bind(key)
                        .execute(&self.pool)
                        .await?;
                    return Ok(None);
                }
            }

            Ok(Some(value))
        } else {
            Ok(None)
        }
    }
}
```

#### RocksDBå­˜å‚¨
```rust
pub struct RocksDBBackend {
    db: Arc<RwLock<rocksdb::DB>>,
    path: PathBuf,
}

impl RocksDBBackend {
    pub fn new(path: PathBuf) -> Result<Self, StorageError> {
        let mut opts = rocksdb::Options::default();
        opts.create_if_missing(true);
        opts.set_max_open_files(1000);
        opts.set_compression_type(rocksdb::DBCompressionType::Lz4);

        let db = rocksdb::DB::open(&opts, &path)?;

        Ok(RocksDBBackend {
            db: Arc::new(RwLock::new(db)),
            path,
        })
    }
}

#[async_trait]
impl StorageBackend for RocksDBBackend {
    async fn put(&self, key: &str, value: &[u8], options: StorageOptions) -> Result<(), StorageError> {
        let db = self.db.read().await;

        // å¤„ç†å‹ç¼©
        let data = if let Some(compression) = options.compression {
            self.compress_data(value, compression).await?
        } else {
            value.to_vec()
        };

        // å¤„ç†åŠ å¯†
        let final_data = if let Some(encryption) = options.encryption {
            self.encrypt_data(&data, encryption).await?
        } else {
            data
        };

        // å­˜å‚¨æ•°æ®
        db.put(key.as_bytes(), final_data)?;

        // è®¾ç½®TTLï¼ˆå¦‚æœæ”¯æŒï¼‰
        if let Some(ttl) = options.ttl {
            // RocksDBæœ¬èº«ä¸æ”¯æŒTTLï¼Œéœ€è¦é¢å¤–çš„å…ƒæ•°æ®ç®¡ç†
            self.set_ttl_metadata(key, ttl).await?;
        }

        Ok(())
    }

    async fn get(&self, key: &str) -> Result<Option<Vec<u8>>, StorageError> {
        let db = self.db.read().await;

        // æ£€æŸ¥TTL
        if self.is_expired(key).await? {
            // åˆ é™¤è¿‡æœŸæ•°æ®
            let _ = db.delete(key.as_bytes());
            return Ok(None);
        }

        if let Some(data) = db.get(key.as_bytes())? {
            // å¤„ç†è§£å¯†
            let mut data = self.decrypt_data(&data).await?;

            // å¤„ç†è§£å‹ç¼©
            data = self.decompress_data(&data).await?;

            Ok(Some(data))
        } else {
            Ok(None)
        }
    }
}
```

### ğŸ“Š æ•°æ®å‹ç¼©å’ŒåŠ å¯† (Data Compression and Encryption)

#### å‹ç¼©ç®—æ³•
```rust
#[derive(Debug, Clone, Copy)]
pub enum CompressionType {
    None,
    Gzip,
    Zstd,
    Lz4,
}

pub struct DataCompressor;

impl DataCompressor {
    pub async fn compress(data: &[u8], compression_type: CompressionType) -> Result<Vec<u8>, CompressionError> {
        match compression_type {
            CompressionType::None => Ok(data.to_vec()),
            CompressionType::Gzip => {
                use flate2::write::GzEncoder;
                use flate2::Compression;
                use std::io::Write;

                let mut encoder = GzEncoder::new(Vec::new(), Compression::default());
                encoder.write_all(data)?;
                Ok(encoder.finish()?)
            }
            CompressionType::Zstd => {
                Ok(zstd::encode_all(data, 3)?)
            }
            CompressionType::Lz4 => {
                Ok(lz4_flex::compress(data))
            }
        }
    }

    pub async fn decompress(data: &[u8], compression_type: CompressionType) -> Result<Vec<u8>, CompressionError> {
        match compression_type {
            CompressionType::None => Ok(data.to_vec()),
            CompressionType::Gzip => {
                use flate2::read::GzDecoder;
                use std::io::Read;

                let mut decoder = GzDecoder::new(data);
                let mut decompressed = Vec::new();
                decoder.read_to_end(&mut decompressed)?;
                Ok(decompressed)
            }
            CompressionType::Zstd => {
                Ok(zstd::decode_all(data)?)
            }
            CompressionType::Lz4 => {
                Ok(lz4_flex::decompress(data, usize::MAX)?)
            }
        }
    }
}
```

#### åŠ å¯†ç®—æ³•
```rust
#[derive(Debug, Clone)]
pub enum EncryptionType {
    None,
    Aes256Gcm { key: Vec<u8>, nonce: Vec<u8> },
    ChaCha20Poly1305 { key: Vec<u8>, nonce: Vec<u8> },
}

pub struct DataEncryptor;

impl DataEncryptor {
    pub async fn encrypt(data: &[u8], encryption_type: EncryptionType) -> Result<Vec<u8>, EncryptionError> {
        match encryption_type {
            EncryptionType::None => Ok(data.to_vec()),
            EncryptionType::Aes256Gcm { key, nonce } => {
                use aes_gcm::{Aes256Gcm, Key, Nonce};
                use aes_gcm::aead::{Aead, NewAead};

                let cipher = Aes256Gcm::new(Key::from_slice(&key));
                let nonce = Nonce::from_slice(&nonce);

                let ciphertext = cipher.encrypt(nonce, data)
                    .map_err(|_| EncryptionError::EncryptionFailed)?;

                Ok(ciphertext)
            }
            EncryptionType::ChaCha20Poly1305 { key, nonce } => {
                use chacha20poly1305::{ChaCha20Poly1305, Key, Nonce};
                use chacha20poly1305::aead::{Aead, NewAead};

                let cipher = ChaCha20Poly1305::new(Key::from_slice(&key));
                let nonce = Nonce::from_slice(&nonce);

                let ciphertext = cipher.encrypt(nonce, data)
                    .map_err(|_| EncryptionError::EncryptionFailed)?;

                Ok(ciphertext)
            }
        }
    }

    pub async fn decrypt(data: &[u8], encryption_type: EncryptionType) -> Result<Vec<u8>, EncryptionError> {
        match encryption_type {
            EncryptionType::None => Ok(data.to_vec()),
            EncryptionType::Aes256Gcm { key, nonce } => {
                use aes_gcm::{Aes256Gcm, Key, Nonce};
                use aes_gcm::aead::{Aead, NewAead};

                let cipher = Aes256Gcm::new(Key::from_slice(&key));
                let nonce = Nonce::from_slice(&nonce);

                let plaintext = cipher.decrypt(nonce, data)
                    .map_err(|_| EncryptionError::DecryptionFailed)?;

                Ok(plaintext)
            }
            EncryptionType::ChaCha20Poly1305 { key, nonce } => {
                use chacha20poly1305::{ChaCha20Poly1305, Key, Nonce};
                use chacha20poly1305::aead::{Aead, NewAead};

                let cipher = ChaCha20Poly1305::new(Key::from_slice(&key));
                let nonce = Nonce::from_slice(&nonce);

                let plaintext = cipher.decrypt(nonce, data)
                    .map_err(|_| EncryptionError::DecryptionFailed)?;

                Ok(plaintext)
            }
        }
    }
}
```

### ğŸ“ˆ ç›‘æ§å’ŒæŒ‡æ ‡ (Monitoring and Metrics)

#### å­˜å‚¨æŒ‡æ ‡æ”¶é›†å™¨
```rust
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StorageMetrics {
    pub backend_name: String,
    pub operations_total: u64,
    pub operations_success: u64,
    pub operations_failed: u64,
    pub avg_response_time: f64,
    pub p95_response_time: f64,
    pub p99_response_time: f64,
    pub throughput_bytes_per_sec: f64,
    pub storage_used_bytes: u64,
    pub storage_available_bytes: u64,
    pub cache_hit_ratio: f64,
    pub compression_ratio: f64,
}

pub struct MetricsCollector {
    metrics: Arc<RwLock<HashMap<String, StorageMetrics>>>,
    histogram: Arc<RwLock<Histogram>>,
}

impl MetricsCollector {
    pub async fn record_operation(&self, operation: &str, backend: &str, success: bool) {
        let mut metrics = self.metrics.write().await;
        let backend_metrics = metrics.entry(backend.to_string()).or_insert(StorageMetrics::default());

        backend_metrics.operations_total += 1;
        if success {
            backend_metrics.operations_success += 1;
        } else {
            backend_metrics.operations_failed += 1;
        }
    }

    pub async fn record_response_time(&self, backend: &str, duration: Duration) {
        let mut metrics = self.metrics.write().await;
        let backend_metrics = metrics.entry(backend.to_string()).or_insert(StorageMetrics::default());

        // æ›´æ–°å“åº”æ—¶é—´ç»Ÿè®¡
        self.histogram.write().await.record(duration.as_millis() as f64);

        // ç®€å•çš„ç§»åŠ¨å¹³å‡
        let alpha = 0.1;
        backend_metrics.avg_response_time = backend_metrics.avg_response_time * (1.0 - alpha) + duration.as_millis() as f64 * alpha;
    }

    pub async fn get_metrics(&self, backend: &str) -> Option<StorageMetrics> {
        let metrics = self.metrics.read().await;
        metrics.get(backend).cloned()
    }
}
```

## æ¶æ„è®¾è®¡

### å­˜å‚¨æŠ½è±¡æ¶æ„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 å­˜å‚¨æŠ½è±¡å±‚ (Storage Layer)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  æ™ºèƒ½è·¯ç”±å™¨  â”‚ â”‚  å­˜å‚¨å®¢æˆ·ç«¯  â”‚ â”‚  æŒ‡æ ‡æ”¶é›†å™¨  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ å†…å­˜å­˜å‚¨     â”‚ â”‚ Rediså­˜å‚¨   â”‚ â”‚ PostgreSQL  â”‚     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ æ–‡ä»¶ç³»ç»Ÿ     â”‚ â”‚ MySQLå­˜å‚¨   â”‚ â”‚ SQLite       â”‚     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Sledå­˜å‚¨    â”‚ â”‚ RocksDB     â”‚ â”‚ è‡ªå®šä¹‰å­˜å‚¨    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              å¾®å†…æ ¸å­˜å‚¨æœåŠ¡å±‚                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ•°æ®æµè®¾è®¡
```
åº”ç”¨è¯·æ±‚ â†’ è·¯ç”±é€‰æ‹© â†’ åç«¯é€‰æ‹© â†’ æ•°æ®å¤„ç† â†’ å­˜å‚¨æ“ä½œ â†’ æŒ‡æ ‡è®°å½•
    â†“         â†“         â†“         â†“         â†“         â†“
  æ™ºèƒ½ç¼“å­˜   ç­–ç•¥è¯„ä¼°   åç«¯è·¯ç”±   å‹ç¼©åŠ å¯†   æŒä¹…åŒ–å­˜å‚¨   æ€§èƒ½ç›‘æ§
  è´Ÿè½½å‡è¡¡   æˆæœ¬ä¼˜åŒ–   å¥åº·æ£€æŸ¥   ä¸€è‡´æ€§ä¿è¯   æ•…éšœæ¢å¤   å‘Šè­¦è§¦å‘
```

## é…ç½®ç®¡ç†

### å­˜å‚¨é…ç½®
```toml
[storage]
default_backend = "redis"
enable_caching = true
cache_ttl_default = 3600

[storage.backends.memory]
max_size = 1073741824  # 1GB

[storage.backends.redis]
url = "redis://localhost:6379"
prefix = "sira"
connection_pool_size = 10

[storage.backends.postgres]
url = "postgres://user:pass@localhost/sira"
table_name = "storage_items"
max_connections = 20

[storage.backends.rocksdb]
path = "./data/rocksdb"
compression = "lz4"
cache_size = 536870912  # 512MB

[storage.routing]
strategy = "intelligent"
rebalance_interval = 300

[storage.compression]
default_algorithm = "zstd"
level = 3

[storage.encryption]
enabled = true
algorithm = "aes256gcm"
key_rotation_interval = 86400

[storage.monitoring]
metrics_collection_interval = 60
alert_on_high_latency = true
alert_on_storage_full = true
```

## æµ‹è¯•å’ŒéªŒè¯

### å­˜å‚¨åç«¯æµ‹è¯•
```rust
#[cfg(test)]
mod backend_tests {
    use super::*;

    #[tokio::test]
    async fn test_memory_backend() {
        let backend = MemoryBackend::new(1024 * 1024); // 1MB

        // æµ‹è¯•åŸºæœ¬æ“ä½œ
        backend.put("key1", b"value1", StorageOptions::default()).await.unwrap();
        let value = backend.get("key1").await.unwrap();
        assert_eq!(value, Some(b"value1".to_vec()));

        // æµ‹è¯•TTL
        let options = StorageOptions {
            ttl: Some(Duration::from_millis(100)),
            ..Default::default()
        };
        backend.put("key2", b"value2", options).await.unwrap();

        // ç­‰å¾…è¿‡æœŸ
        tokio::time::sleep(Duration::from_millis(200)).await;
        let expired_value = backend.get("key2").await.unwrap();
        assert_eq!(expired_value, None);
    }

    #[tokio::test]
    async fn test_redis_backend() {
        let backend = RedisBackend::new("redis://localhost:6379", "test".to_string()).unwrap();

        // æµ‹è¯•åŸºæœ¬æ“ä½œ
        backend.put("key1", b"value1", StorageOptions::default()).await.unwrap();
        let value = backend.get("key1").await.unwrap();
        assert_eq!(value, Some(b"value1".to_vec()));

        // æµ‹è¯•å‹ç¼©
        let options = StorageOptions {
            compression: Some(CompressionType::Gzip),
            ..Default::default()
        };
        backend.put("key2", b"compress me", options).await.unwrap();
        let compressed_value = backend.get("key2").await.unwrap();
        assert_eq!(compressed_value, Some(b"compress me".to_vec()));
    }

    #[tokio::test]
    async fn test_rocksdb_backend() {
        let temp_dir = tempfile::tempdir().unwrap();
        let backend = RocksDBBackend::new(temp_dir.path().to_path_buf()).unwrap();

        // æµ‹è¯•åŸºæœ¬æ“ä½œ
        backend.put("key1", b"value1", StorageOptions::default()).await.unwrap();
        let value = backend.get("key1").await.unwrap();
        assert_eq!(value, Some(b"value1".to_vec()));

        // æµ‹è¯•æ‰¹é‡æ“ä½œ
        let operations = vec![
            BatchOperation::Put { key: "batch1".to_string(), value: b"batch_value1".to_vec(), options: StorageOptions::default() },
            BatchOperation::Put { key: "batch2".to_string(), value: b"batch_value2".to_vec(), options: StorageOptions::default() },
        ];

        let results = backend.batch(operations).await.unwrap();
        assert_eq!(results.len(), 2);
        assert!(results.iter().all(|r| matches!(r, BatchResult::Success)));
    }
}
```

### è·¯ç”±æµ‹è¯•
```rust
#[cfg(test)]
mod routing_tests {
    use super::*;

    #[tokio::test]
    async fn test_performance_based_routing() {
        let router = StorageRouter::new();

        // æ¨¡æ‹Ÿåç«¯ç»Ÿè®¡
        let mut stats = HashMap::new();
        stats.insert("fast_backend".to_string(), BackendStats {
            avg_response_time: 10.0,
            ..Default::default()
        });
        stats.insert("slow_backend".to_string(), BackendStats {
            avg_response_time: 100.0,
            ..Default::default()
        });

        // æµ‹è¯•è·¯ç”±é€‰æ‹©
        let backend = router.performance_based_routing("test_key", &StorageOptions::default(), &stats).await.unwrap();
        assert_eq!(backend, "fast_backend");
    }

    #[tokio::test]
    async fn test_data_type_routing() {
        let router = StorageRouter::new();

        // é…ç½®æ•°æ®ç±»å‹æ¨¡å¼
        router.add_data_pattern("cache:*".to_string(), DataPattern {
            preferred_backend: "redis".to_string(),
            ..Default::default()
        });

        router.add_data_pattern("data:*".to_string(), DataPattern {
            preferred_backend: "postgres".to_string(),
            ..Default::default()
        });

        // æµ‹è¯•è·¯ç”±
        let cache_backend = router.select_backend("cache:user:123", &StorageOptions::default()).await.unwrap();
        assert_eq!(cache_backend, "redis");

        let data_backend = router.select_backend("data:document:456", &StorageOptions::default()).await.unwrap();
        assert_eq!(data_backend, "postgres");
    }
}
```

### é›†æˆæµ‹è¯•
```rust
#[cfg(test)]
mod integration_tests {
    use super::*;

    #[tokio::test]
    async fn test_storage_client_integration() {
        let client = StorageClient::new();

        // æ³¨å†Œåç«¯
        let memory_backend = Arc::new(MemoryBackend::new(1024 * 1024));
        let redis_backend = Arc::new(RedisBackend::new("redis://localhost:6379", "test".to_string()).unwrap());

        client.register_backend("memory", memory_backend).await.unwrap();
        client.register_backend("redis", redis_backend).await.unwrap();

        // æµ‹è¯•æ™ºèƒ½å­˜å‚¨
        client.store("memory_key", b"memory_value", StorageOptions::default()).await.unwrap();
        client.store("redis_key", b"redis_value", StorageOptions::default()).await.unwrap();

        // æµ‹è¯•æ™ºèƒ½æ£€ç´¢
        let memory_value = client.retrieve("memory_key").await.unwrap();
        assert_eq!(memory_value, Some(b"memory_value".to_vec()));

        let redis_value = client.retrieve("redis_key").await.unwrap();
        assert_eq!(redis_value, Some(b"redis_value".to_vec()));
    }

    #[tokio::test]
    async fn test_cross_backend_operations() {
        let client = StorageClient::new();

        // æ³¨å†Œå¤šä¸ªåç«¯
        // ...

        // æµ‹è¯•æ•°æ®è¿ç§»
        client.migrate_data("source_backend", "target_backend", "migration_pattern").await.unwrap();

        // éªŒè¯æ•°æ®è¿ç§»
        let migrated_value = client.retrieve("migrated_key").await.unwrap();
        assert_eq!(migrated_value, Some(b"migrated_value".to_vec()));
    }
}
```

## éƒ¨ç½²å’Œè¿ç»´

### å®¹å™¨åŒ–éƒ¨ç½²
```dockerfile
FROM rust:1.70-slim as builder
WORKDIR /app
COPY . .
RUN cargo build --release --bin sira-storage-backends

FROM debian:bookworm-slim
RUN apt-get update && apt-get install -y ca-certificates
COPY --from=builder /app/target/release/sira-storage-backends /usr/local/bin/

# åˆ›å»ºæ•°æ®ç›®å½•
RUN mkdir -p /app/data
VOLUME ["/app/data"]

EXPOSE 9095
CMD ["sira-storage-backends"]
```

### å­˜å‚¨é›†ç¾¤éƒ¨ç½²
```yaml
version: '3.8'
services:
  storage-node-1:
    image: sira/storage:latest
    environment:
      - NODE_ID=1
      - CLUSTER_NODES=storage-node-1,storage-node-2,storage-node-3
    volumes:
      - ./data/node1:/app/data
    networks:
      - storage-network

  storage-node-2:
    image: sira/storage:latest
    environment:
      - NODE_ID=2
      - CLUSTER_NODES=storage-node-1,storage-node-2,storage-node-3
    volumes:
      - ./data/node2:/app/data
    networks:
      - storage-network

  storage-node-3:
    image: sira/storage:latest
    environment:
      - NODE_ID=3
      - CLUSTER_NODES=storage-node-1,storage-node-2,storage-node-3
    volumes:
      - ./data/node3:/app/data
    networks:
      - storage-network

networks:
  storage-network:
    driver: bridge
```

### ç›‘æ§å‘Šè­¦
- **å­˜å‚¨å¥åº·æ£€æŸ¥**: å®šæœŸæ£€æŸ¥å„åç«¯è¿æ¥å’Œæ€§èƒ½
- **å®¹é‡ç›‘æ§**: ç›‘æ§å­˜å‚¨ä½¿ç”¨ç‡å’Œå¢é•¿è¶‹åŠ¿
- **æ€§èƒ½ç›‘æ§**: å“åº”æ—¶é—´ã€ååé‡ã€é”™è¯¯ç‡ç›‘æ§
- **æ•°æ®ä¸€è‡´æ€§**: è·¨åç«¯æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
- **å¤‡ä»½çŠ¶æ€**: å¤‡ä»½ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€ç›‘æ§

## å®‰å…¨è€ƒè™‘

### æ•°æ®ä¿æŠ¤
- **ä¼ è¾“åŠ å¯†**: TLSåŠ å¯†çš„å­˜å‚¨è¿æ¥
- **æ•°æ®åŠ å¯†**: æ”¯æŒå¤šç§åŠ å¯†ç®—æ³•
- **å¯†é’¥ç®¡ç†**: å®‰å…¨çš„å¯†é’¥å­˜å‚¨å’Œè½®æ¢
- **è®¿é—®æ§åˆ¶**: åŸºäºè§’è‰²çš„å­˜å‚¨è®¿é—®æ§åˆ¶

### éšç§ä¿æŠ¤
- **æ•°æ®è„±æ•**: æ•æ„Ÿæ•°æ®çš„è‡ªåŠ¨è„±æ•
- **å®¡è®¡æ—¥å¿—**: å®Œæ•´çš„å­˜å‚¨æ“ä½œå®¡è®¡
- **æ•°æ®ä¿ç•™**: å¯é…ç½®çš„æ•°æ®ä¿ç•™ç­–ç•¥
- **åˆè§„æ”¯æŒ**: GDPRã€CCPAç­‰åˆè§„æ ‡å‡†æ”¯æŒ

## æ‰©å±•æœºåˆ¶

### è‡ªå®šä¹‰å­˜å‚¨åç«¯
```rust
pub struct CustomBackend {
    // è‡ªå®šä¹‰å­˜å‚¨é€»è¾‘
}

#[async_trait]
impl StorageBackend for CustomBackend {
    async fn put(&self, key: &str, value: &[u8], options: StorageOptions) -> Result<(), StorageError> {
        // è‡ªå®šä¹‰å­˜å‚¨å®ç°
        Ok(())
    }

    async fn get(&self, key: &str) -> Result<Option<Vec<u8>>, StorageError> {
        // è‡ªå®šä¹‰æ£€ç´¢å®ç°
        Ok(None)
    }
}

// æ³¨å†Œè‡ªå®šä¹‰åç«¯
client.register_backend("custom", Arc::new(CustomBackend::new())).await?;
```

### è‡ªå®šä¹‰è·¯ç”±ç­–ç•¥
```rust
pub struct CustomRoutingStrategy;

#[async_trait]
impl RoutingStrategy for CustomRoutingStrategy {
    async fn select_backend(&self, key: &str, options: &StorageOptions, stats: &HashMap<String, BackendStats>) -> Result<String, RouterError> {
        // è‡ªå®šä¹‰è·¯ç”±é€»è¾‘
        Ok("selected_backend".to_string())
    }
}

// æ³¨å†Œè‡ªå®šä¹‰ç­–ç•¥
router.add_strategy(Box::new(CustomRoutingStrategy)).await?;
```

## æœªæ¥è§„åˆ’

### ğŸš€ å¢å¼ºåŠŸèƒ½
- [ ] åˆ†å¸ƒå¼å­˜å‚¨é›†ç¾¤
- [ ] å¯¹è±¡å­˜å‚¨é›†æˆ
- [ ] å®æ—¶æ•°æ®æµå¤„ç†
- [ ] å­˜å‚¨ç­–ç•¥AIä¼˜åŒ–
- [ ] å¤šåŒºåŸŸæ•°æ®å¤åˆ¶

### âš¡ æ€§èƒ½ä¼˜åŒ–
- [ ] å­˜å‚¨åˆ†å±‚ç¼“å­˜
- [ ] æ™ºèƒ½æ•°æ®é¢„å–
- [ ] å‹ç¼©ç®—æ³•ä¼˜åŒ–
- [ ] è¿æ¥æ± ä¼˜åŒ–
- [ ] æ‰¹é‡æ“ä½œä¼˜åŒ–

### ğŸ›¡ï¸ ä¼ä¸šçº§ç‰¹æ€§
- [ ] æ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†
- [ ] å­˜å‚¨ç­–ç•¥åˆè§„
- [ ] å¤šç§Ÿæˆ·æ•°æ®éš”ç¦»
- [ ] ä¼ä¸šçº§å®‰å…¨å®¡è®¡
- [ ] ç¾éš¾æ¢å¤æœºåˆ¶

---

**Sira Storage Backends** - ç»Ÿä¸€çš„å¤šåç«¯å­˜å‚¨æŠ½è±¡
